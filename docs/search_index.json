[
["index.html", "Interfacing R with Web Technologies for Data Acquistion and Interactive Visualization Abstract", " Interfacing R with Web Technologies for Data Acquistion and Interactive Visualization Carson Sievert 2016-08-24 Abstract The following describes a collection of software interfaces for data acquisiton and visualization. All of these interfaces are freely available as extension packages to the R language and leverage web technologies to achieve accessible, portable, and reproducible workflows. The majority of this work (LDAvis, animint, and plotly) focuses on interactive visualization. These interfaces fall roughly into two categories: (1) domain-specific (LDAvis) and (2) general purpose visualization (animint and plotly). More specifially, the LDAvis package produces an interactive visualization to aid interpretation of Latent Dirichlet Allocation (LDA) model output. The animint and plotly packages are more general, and build upon principles from the grammar of graphics (Wilkinson 2005), but extend those principles in slightly different ways to enable interactivity, such as brushing a scatterplot matrix (Becker and Cleveland 1987). References "],
["what-makes-a-good-software-interface.html", "0.1 What makes a good software interface?", " 0.1 What makes a good software interface? Unwin and Hofmann (Unwin and Hofmann 2009) discuss the strengths, weaknesses, and differences between using graphical and command-line interfaces for data analysis. Graphical user interfaces (GUIs) can be much more intuitive to use, but at the cost of being less flexible, precise, and repeatable. Unwin and Hofmann argue statistical software should strive to achieve a synergy of two that leverages both of their strengths. That is, a command-line interface when we can precisely describe what we want and a graphical interface for “searching for information and interesting structures without fully specified questions.” Unwin and Hofmann further discuss the different audiences these interfaces attract. Command-line interfaces typically attract “power users” such as applied statisticians and statistical researchers in a university, whereas more casual users of statistical software typically prefer a GUI. In later sections, we discuss GUIs in greater detail within the context of interactive statistical graphics. For now, we briefly discuss some best practices for designing a command-line interface for statistical computing in R. Before authoring an interface, one should establish the target audience, the class of problems it should address, and loosely define how the interface should actually work. During this process, it may also be helpful to identify your audience as being primarily composed of software developers or data analysts. Developers are typically more interested in using the interface to develop novel software or incorporating the functionality into a larger scientific computing environment (Jereon Ooms 2014). In this case, interactive exploration and troubleshooting is not always a luxury, so robust functionality is of utmost importance. On the other hand, analysts interfaces should work well in an interactive environment since this caters to rapid prototyping of ideas and troubleshooting of errors. Good developer interfaces often make it easier to implement good analyst interfaces. A great recent example of a good developer interface is the R package Rcpp, which provides a seamless interface between R with C++ (Eddelbuettel 2013). To date, more than 500 R packages use Rcpp to make interfaces that are both expressive and efficient, including the highly influential analyst interfaces such as tidyr and dplyr (Wickham 2014); (Wickham and Francois 2015). These interfaces help analysts focus on the primary task of wrangling data into a form suitable for visualization and statistical modeling, rather than focusing on the implementation details behind how the transformations are performed. (Donoho 2015) argues that these interfaces “May have more impact on today’s practice of data analysis than many highly-regarded theoretical statistics papers”. Evaluating statistical computing interfaces is certainly a subjective matter since we all have different tastes, different backgrounds, and have different needs. It seems reasonable to evaluate an interface based on its effectiveness and efficiency in aiding a user complete their task, but as (Unwin and Hofmann 2009) points out, “There is a tendency to judge software by the most powerful tools they provide (whether with a good interface or not)”. As a result, all too often, analysts must spend time gaining the skills of a software developer. Good analyst interfaces often abstract functionality from developer interfaces in a way that allow analysts to focus on their primary task of acquiring/analyzing/modeling/visualizing data, rather than the implementation details. The following focuses on such work with respect to acquiring data from the web and interactive statistical web graphics. --> References "],
["acquiring-and-wrangling-web-content-in-r.html", "1 Acquiring and wrangling web content in R ", " 1 Acquiring and wrangling web content in R "],
["interfaces-for-working-with-web-content.html", "1.1 Interfaces for working with web content", " 1.1 Interfaces for working with web content R has a rich history of interfacing with web technologies for accomplishing a variety of tasks such as requesting, manipulating, and creating web content. As an important first step, extending ideas from (Chambers 1999), Brian Ripley implemented the connections interface for file-oriented input/output in R (Ripley 2001). This interface supports a variety of common transfer protocols (HTTP, HTTPS, FTP), providing access to most files on the web that can be identified with a Uniform Resource Locator (URL). Connection objects are actually external pointers, meaning that, instead of immediately reading the file, they just point to the file, and make no assumptions about the actual contents of the file. Many functions in the base R distribution for reading data (e.g., scan, read.table, read.csv, etc.) are built on top of connections, and provide additional functionality for parsing well-structured plain-text into basic R data structures (vector, list, data frame, etc.). However, the base R distribution does not provide functionality for parsing common file formats found on the web (e.g., HTML, XML, JSON). In addition, the standard R connection interface provides no support for communicating with web servers beyond a simple HTTP GET request (Lang 2006). The RCurl, XML, and RJSONIO packages were major contributions that drastically improved our ability to request, manipulate, and create web content from R (Nolan and Temple Lang 2014). The RCurl package provides a suite of high and low level bindings to the C library libcurl, making it possible to transfer files over more network protocols, communicate with web servers (e.g., submit forms, upload files, etc.), process their responses, and handle other details such as redirects and authentication (Temple Lang 2014a). The XML package provides low-level bindings to the C library libxml2, making it possible to download, parse, manipulate, and create XML (and HTML) (Temple Lang and CRAN Team 2015). To make this possible, XML also provides some data structures for representing XML in R. The RJSONIO package provides a mapping between R objects and JavaScript Object Notation (JSON) (Temple Lang 2014b). These packages were heavily used for years, but several newer interfaces have made these tasks easier and more efficient. The curl, httr, and jsonlite packages are more modern R interfaces for requesting content on the web and interacting with web servers. The curl package provides a much simpler interface to libcurl that also supports streaming data (useful for transferring large data), and generally has better performance than RCurl (Ooms 2015). The httr package builds on curl and organizes it’s functionality around HTTP verbs (GET, POST, etc.) (Wickham 2015a). Since most web application programming interfaces (APIs) organize their functionality around these same verbs, it is often very easy to write R bindings to web services with httr. The httr package also builds on jsonlite since it provides consistent mappings between R/JSON and most most modern web APIs accept and send messages in JSON format (Jeroen Ooms 2014a). These packages have already had a profound impact on the investment required to interface R with web services, which are useful for many things beyond data acquisition. For example, it is now easy to install R packages hosted on the web (devtools), perform cloud computing (analogsea), and archive/share computational outputs (dvn, rfigshare, RAmazonS3, googlesheets, rdrop2, etc.). The rvest package builds on httr and makes it easy to manipulate content in HTML/XML files (Wickham 2015c). Using rvest in combination with SelectorGadget, it is often possible to extract structured information (e.g., tables, lists, links, etc) from HTML with almost no knowledge/familiarity with web technologies. The XML2R package has a similar goal of providing an interface to acquire and manipulate XML content into tabular R data structures without any working knowledge of XML/XSLT/XPath (Sievert 2014b). As a result, these interfaces reduce the startup costs required for analysts to acquire data from the web. Packages such as XML, XML2R, and rvest can download and parse the source of web pages, which is static, but extracting dynamic web content requires additional tools. The R package rdom fills this void and makes it easy to render and access the Document Object Model (DOM) using the headless browsing engine phantomjs (Sievert 2015a). The R package RSelenium can also render dynamic web pages and simulate user actions, but its broad scope and heavy software requirements make it harder to use and less reliable compared to rdom (Harrison 2014). rdom is also designed to work seamlessly with rvest, so that one may use the rdom() function instead of read_html() to render, parse, and return the DOM as HTML (instead of just the HTML page source). Any combination of these R packages may be useful in acquiring data for personal use and/or providing a higher-level interface to specific data source(s) to increase their accessibility. The next section focuses on such interfaces. References "],
["interfaces-for-acquiring-data-on-the-web.html", "1.2 Interfaces for acquiring data on the web", " 1.2 Interfaces for acquiring data on the web The web provides access to the world’s largest repository of publicly available information and data. This provides a nice potential resource both teaching and practicing applied statistics, but to be practical useful, it often requires a custom interface to make data more accessible. If publishers follow best practices, a custom interface to the data source usually is not needed, but this is rarely the case. Many times structured data is embedded within larger unstructured documents, making it difficult to incorporate into a data analysis workflow. This is especially true of data used to inform downstream web applications, typically in XML and/or JSON format. There are two main ways to make such data more accessible: (1) package, document, and distribute the data itself (2) provide functionality to acquire the data. If the data source is fairly small, somewhat static, and freely available with an open license, then we can directly provide data via R packaging mechanism. In this case, it is best practice for package authors include scripts used to acquire, transform, and clean the data. This model is especially nice for both teaching and providing examples, since users can easily access data by installing the R package. (Wickham 2015b) provides a nice section outlining the details of bundling data with R packages.1 R packages that just provide functionality to acquire data can be more desirable than bundling it for several reasons. In some cases, it helps avoid legal issues with rehosting copyrighted data. Furthermore, the source code of R packages can always be inspected, so users can verify the cleaning and transformations performed on the data to ensure its integrity, and suggest changes if necessary. They are also versioned, which makes the data acquisition, and thus any downstream analysis, more reproducible and transparent. It is also possible to handle dynamic data with such interfaces, meaning that new data can be acquired without any change to the underlying source code. As explained in Taming PITCHf/x Data with XML2R and pitchRx, this is an important quality of the pitchRx R package since new PITCHf/x data is made available on a daily basis. Perhaps the largest centralized effort in this domain is lead by rOpenSci, a community of R developers that, at the time of writing, maintains more than 50 packages providing access to scientific data ranging from bird sightings, species occurrence, and even text/metadata from academic publications. This provides a tremendous service to researchers who want to spend their time building models and deriving insights from data, rather than learning the programming skills necessary to acquire and clean it. It’s becoming increasingly clear that “meta” packages that standardize the interface to data acquisition/curation in a particular domain would be tremendously useful. However, it is not clear how such interfaces should be designed. The R package etl is one step in this direction and aims to provide a standardized interface for any data access package that fits into an Extract-Transform-Load paradigm (Baumer and Sievert, n.d.). The package provides generic extract-transform-load functions, but requires package authors to write custom extract-transform methods for the specific data source. In theory, the default load method works for any application; as well as other database management operations such as update and clean. TODO: MERGE IDEAS FROM THE “CHAPTER” ABOVE AND BELOW INTO A COHESIVE CHAPTER!!! --> References "],
["curating-open-data-in-r.html", "2 Curating Open Data in R", " 2 Curating Open Data in R 2.0.1 The Rise of Open Data The World Wide Web brought about exciting new opportunities to publicly share information and data. Not all data on the Web is publicly available and free to use, but the data that is, is commonly referred to as “Open Data”. Open data can be a controversial topic, but it can also be constructive, especially for the academic community, where verified conclusions maintain integrity and enable progress of the discipline. In fact, in recent years, we’ve seen several high profile works refuted, after they were published, when errors in their data analysis were identified (see (Baggerly and Coombes 2009); (Herndon, Ash, and Pollin 2014)). Open data will not, in itself, prevent wrong conclusions from being made, but it does gives us a greater potential to identify them. It has been shown that researchers are generally willing to share their data, but the movement has been slow largely due to a lack of “systems that make it quick and easy to share data” (Tenopir et al. 2011); (Pampel 2013). Fortunately, there are a number of efforts to reduce this burden and make research data repositories more visible, usable, and worthwhile (Pampel et al. 2013); (King 2007). These efforts would certainly make the discovery and acquisition of open data much easier – a problem that is commonly overlooked, and has the potential of preventing analysis altogether. (Stuart Dillon 2013) proposed a general search engine for discovering and acquiring data on the Web, but assumes data source(s) have been pre-identified and neatly organized into a relational form by domain experts. This approach might work in a perfect world where volunteers are abundant and publishers are aware of the analysis others may want to perform on the data, but it generally doesn’t work for a Web that is constantly expanding, evolving, and increasing in complexity. The applications of open data are not restricted in any way research experiments or academic research. From Wikipedia articles to Government records, the Web hosts the largest and most diverse set of publicly available data. Unfortunately, this data is often difficult to acquire and/or embedded within unstructured documents, making it difficult to incorporate into a data analysis workflow. This is especially true of data used to inform downstream Web applications. Thankfully, many skilled programmers spend many hours building a wide variety of tools to provide more convenient access to open data. A categorization of freely available tools designed to solve these problems is presented in Working with Open Data in R. 2.0.2 What Makes Data on the Web “Open”? The ability to access and acquire data on the Web does not grant one permission to use it in whatever way they want. If the data is published without a license, it is owned by the publisher under copyright, and one must ask permission to use the data for their intended purpose. If the data is published with a license, that license will dictate the terms of use. Truly open data is published with a license, dedicating it to the public domain which waives ownership of copyright. Even if the license does not waive copyright, it will typically allow for individuals to use the data for non-commercial purposes. 2.0.3 On the Quality, Quantity, and Accessibility of Open Data “The Web is rarely perfectly honest, complete, and unbiased; but it’s still pretty damn useful.” - (Swartz 2013) Extracting data placed within HTML &lt;table&gt; tags is often trivial, but they can contain uninteresting information from a data analysis perspective. In 2008, (Cafarella et al. 2008) estimated that 154 million HTML tables (out of the 14.1 billion considered) contained high quality relational data (TODO: what exactly do they mean by high quality?). Other studies have estimated the rate of “genuine” HTML tables to be around 15.2 percent (Wang and Hu 2002). Genuine structured data certainly exists outside of HTML tables, sometimes in the form of unstructured text or lists, making it difficult to automatically detect and acquire. There are a number of algorithms for automatic acquisition of data on the Web (Crescenzi, Mecca, and Merialdo 2001);(Ortona et al. 2015). We’ve also seen a number of free and paid Web services such as http://import.io and http://enigma.io implement such algorithms and as well as add crowd-sourcing features. These automated approaches typically work well in “nice” cases where HTML pages are static, sensibly structured, and consist mostly of data. In practice, these assumptions typically don’t hold, and a extraction rule specific to the data source is required. An overview of tools for writing such wrappers in the R language is presented in Extracting Open Data with R. Assuming that a data source has been identified and extracted, in many cases, that data has to be reshaped and/or cleaned so it’s suitable for use in downstream statistical analysis. There are a number of interactive systems for performing such “data munging” tasks (see (S. K. A. A. P. A. J. H. A. J. Heer 2011); (Raman and Hellerstein 2001); (Verborgh and Wilde 2013)). These systems are especially helpful for discovering unknown problems with the data, but the data munging steps should eventually be programmed so they can be repeated faithfully and scale to a large number of tasks. An overview of tools for writing such wrappers in the R language is presented in Munging Open Data with R. From here on, we refer to the cumulative process of identifying, extracting, munging, linking, and storing data on the Web as curating data. The R language, typically known for it’s statistical modeling capabilities, provides a pragmatic set of tools for facilitating the data curation process and are covered in . 2.0.4 Best practices for publishing open data 2.0.5 Preserving Open Data One valid concern when working with data on the Web is, “What if that resource goes missing?”. This points out a major weakness in the design of HTTP, the transfer protocol of the Web. When we request a page on the Web, HTTP needs to look-up a specific file on a specific machine. If that file changes location, or if the Web server goes down for any reason, that file could be lost, in some cases, forever. Of course, one can make copies of a known, existing resource(s) to keep backup(s) of data. For instance, one could run rsync (or similar) to keep local files in sync with files on another machine. However, rsync doesn’t provide a mechanism for curating data from unstructured files, version control, or collaborating with others. These problems are currently being addressed by the open source project dat which borrow some algorithm design ideas from the revision control system git, but optimizes them to tabular data rather than source code. These tools provide a decent approach to preserving open data assuming their resource locations are known before being removed. The Internet Archive’s Wayback Machine https://archive.org/web is a gigantic effort to archive the Web. In October 2012, its archives topped 10 petabytes of data (Brown 2006), and three years later, a reported 439 billion web pages are available. A number of interesting projects are aimed at discovering and searching content in this archive (Lin, Gholami, and Rao 2014). Although a step in the right direction, the Internet Archive currently adheres to the Robots Exclusion Protocol, so sites requesting Web Robots to ignore their content will not be archived. IPFS is a proposed alternative to HTTP meant to address it’s weaknesses (Benet 2014). IPFS is content addressable, meaning that requests reference the actual content rather than a specific file on a specific machine. IPFS is also de-centralized, meaning that as long as one machine on the network has the content, it can be requested anyone on the network. Although IPFS implements many great ideas, given the massive amount of technology built on top of HTTP, it’s hard to imagine that it will ever go away completely. Thus, if we want to curate open data in order to facilitate more complex data analysis workflows, we need better software tools to do so. https://ipfs.io/ipfs/QmNhFJjGcMPqpuYfxL62VVB9528NXqDNMFXiqN5bgFYiZ1/its-time-for-the-permanent-web.html --> 2.0.6 R as a Data Curation Engine Building off the work of (Chambers 1999) and (Veillard 2006), the R Development Core Team included a number of convenient options within base R to download, and in some cases parse, files via HTTP/FTP. Assuming files are in plain text format, and contain properly formatted, tabular data; high-level functions such as read.table() and read.csv() can load data into R directly from a Uniform Resource Identifier (URI). This is a strong file format assumption, but there are a number of R packages aimed at input/output for different formats (R Core Team 2015); (Wickham and Miller 2015). (Chan, Chan, and Leeper 2015) combines the capabilities of many of these packages into standard interface. References "],
["chapter-2-tools-for-curating-open-data-with-r.html", "3 Chapter 2: Tools for Curating Open Data with R", " 3 Chapter 2: Tools for Curating Open Data with R If data isn’t conveniently accessible as a tabular text file (such as csv or tsv), working with open data typically requires some knowledge of a constantly evolving set of network protocols and Web technologies. This presents a barrier to access for many researchers, but there is a large effort to lower the barrier, especially for the R project. The Web Technologies and Service CRAN Task View does a great job listing all of these efforts for the R project (Chamberlain et al. 2015). Most of these tools can be grouped into one of three categories: A high level interface with direct access to data ready for statistical analysis. In this case, the interface is typically restricted to a single data source, but users can obtain tidy data (Wickham 2014) without any knowledge of protocols or Web Technologies. Some interfaces, such as pitchRx (Sievert 2014b), perform all the steps of data curation under the hood and require a tremendous amount of work by the author. Other interfaces may simply wrap an existing web API for accessing already tidy data. A grammar for transforming non-tidy information into a tidy form. In this case, the tool is typically restricted to a specific file format such as HTML, XML or JSON. However, in some cases, it can remove any requirements/skills required for transforming these file formats. For example, XML2R (Sievert 2014b) makes it possible to transform XML into a tidy form without XPATH and can make it easier build and maintain interfaces that fall under (1) (such as pitchRx and bbscrapeR). A low level interface for working with network protocols and Web Technologies. Using these interfaces require an understanding of popular network protocols such as HTTP/HTTPS, data formats such as JSON and XML, and Web technologies such as HTML/JavaScript/etc. Tools that fall under (1) and (2) build on top of tools under (3). This write-up focuses on how to build tools under (1) and (2) using tools under (3)? Some of the lower-level tools require knowledge of technologies such as HTTP A couple projects, such as XML2R (Sievert 2014b) or tidyjson (needs citation) provide a high-level grammar for transforming . A more common, but less generic, approach is to provide a direct interface to specific data source(s) – one such example is . In some cases, this approach isn’t ideal, since it may require assumptions about the analysis to be performed. Better tools might also encourage statisticians to become more hands-on during the acquisition, transformation, and cleaning of data. This is important for reasons similar to why it’s important to involve a statistician in the design of an experiment. A number of decisions made during data curation can effect data quality. In fact, it may be that a very large portion of the available data is bad, irrelevant to the analysis, and/or simply too large to preserve in bulk. Having a solid foundation in statistical modeling certainly helps in resolving these issues and can lead to more accurate conclusions. 3.0.0.1 Packaging Access to Specific Data Sources R users are probably familiar with the datasets package (distributed with R itself) which provides direct access to classic datasets such as iris or mtcars. R’s packaging mechanism provides a very convenient way to distribute datasets, but it does come with some limitations, which presents problems for packaging open data sources: CRAN imposes size restrictions on the size of the package. CRAN imposes limitations on the frequency of package updates. Some open data sources have terms of use which include restrictions on redistributing the data itself. To avoid these problems, we’ve seen a number of R packages which provide function(s) to extract open data from it’s source, clean and transform it into a usable form. One example of this approach is the pitchRx package where users just have to provide a range of dates to acquire data: library(pitchRx) dat &lt;- scrape(start = &quot;2008-01-01&quot;, end = Sys.Date()) Since the data source contains a number of observational units, the scrape() function returns a list of data frames (one data frame for each unit). These can easily be exported as database tables using a database of the users choice via R’s database interface (Databases 2014). # start a database connection con &lt;- DBI::dbConnect(RSQLite::SQLite(), &quot;mydb.sqlite3&quot;) # extract, transform, and load into a database scrape(start = &quot;2008-01-01&quot;, end = Sys.Date(), con = con) It’s worth noting that the three main jobs of the scrape() function is to extract, transform, and (optionally) load data into a database. These are three general operations that similar packages probably want to perform. Although the extract and transform steps will vary dramatically from package to package, the etl package provides a common interface to promote consistency across packages (Baumer and Sievert, n.d.). 3.0.0.2 Extract, Transform, and Load Paradigm It is all too easy for statistical thinking to get swamped by programming tasks. (Ripley and Ripley 2001) etl is to bbscrapeR as DBI is to RSQLite in the sense that etl and DBI provide a set of generic functions while bbscrapeR and RSQLite implement methods specific to an application. In other words, bbscrapeR implements methods for etl generics that extract, transform, and load a specific (basketball) data source and RSQLite implements methods for DBI generics that allow one to communicate with a MySQL database from R. 3.0.0.3 Working with Popular Web Data Formats Although many formats exist, the majority of data transferred over the web comes in two forms: XML and JSON (needs citation). These formats are designed to be machine readable. That is, given a set of directions, a computer can store and parse information within these files. These formats are great for use in web applications where machines communicate and transfer data. In many cases, this data has valuable information that humans want to analyze. Before that can happen, however; analysts typically have to request and transform HTTP responses into forms suitable for analysis and/or visualization. 3.0.0.4 Requesting Web Content {#sec: request} Before transforming XML/JSON data structures, one typically requests content from a web server via a communication protocol. The most ubiquitous protocol is the Hypertext Transfer Protocol (HTTP). Base R has built-in utilities for reading data (or GETting) via HTTP, but this use case is quite limited. For example, if the analyst wishes to obtain data over a secure connection, such as HTTP Secure (HTTPS), other methods must be used. cURL is a widely used command line tool which covers many protocols for transferring data between machines. The RCurl package provides a low-level interface to cURL with some additional tools for processing (Temple Lang 2014a) for transferring data between machines using Uniform Resource Locators (URLs) and . RCurl httr does XML/rvest fit here? 3.0.0.5 Transforming Web Content into Structured Data {#sec: transform} Working directly with XML/JSON presents challenges for data analysis and statistical modeling. The XML/JSON specifications allow for deeply nested and non-relational data structures; however, popular statistical computing software assumes data exists in a tabular format – each row represents the observational unit and each column represents attributes associated with each observation (Wickham 2014). A number of efforts exist for working with XML/JSON, but in many cases, there is no standard or well-defined way to transform these data structures into a tabular/tidy format. As a result, the analyst is left to handle reshaping of the data into a usable format for data analysis. introduce tidy data framework? XML2R Pros: Easy to express how to go from “unstructured” XML to tidy data. Cons: Must be able to pull XML into memory (consequence of the implementation, not of the concept itself) Future work: C++ backend to do dplyr-esque lazy computations? (TODO: look at connecting to a XML database) 3.0.0.6 Cleaning data for statistical analysis https://github.com/data-cleaning/editrules https://github.com/data-cleaning/validate 3.0.0.7 Working with databases DBI dplyr ============================================== The CRAN task view on Open Data does a great job of summarizing of R clients which connect to these and other open data efforts. Prerequisites to performing analysis on this data might require the discovery, parsing, cleaning, transformation, and integration of data with other sources. A number of disciplines have produced substantial works aimed at simplifying various stages of the data curation process. Although some tasks can be automated or guided by clever systems, at some level, human intervention is required during the curation process. Curation typically requires a custom set of instructions for extracting and structuring information for downstream analysis. The domain knowledge required to write these instructions varies wildly depending on the application. The contribution of this work is an overview of modern tools for curating data from the Web using R. An emphasis is placed on problems that data-driven researchers are likely to face and solutions that minimize the amount of prior knowledge and cognitive effort required. The goal is provide a taxonomy of practical problems and solutions that the community finds useful. References "],
["scraping-dynamic-web-pages.html", "3.1 Scraping Dynamic Web Pages", " 3.1 Scraping Dynamic Web Pages Most Web scraping packages cannot render the DOM (RCurl, XML, xml2, httr, etc.) These packages can (rdom, RSelenium) RSelenium -->"],
["taming-pitchfx-data-with-xml2r-and-pitchrx.html", "4 Taming PITCHf/x Data with XML2R and pitchRx", " 4 Taming PITCHf/x Data with XML2R and pitchRx Pitch f/x refers a massive, publicly available baseball dataset hosted on the web in XML and JSON format. Since this data is large, increases on a daily basis, and only licensed for individual use, the pitchRx package provides a simple interface to download, parse, clean, and transform the data from its source (instead of directly distributing the data). If acquiring large amounts of data, to avoid memory limitations, users may divert incoming data in chunks to a database using any valid R database connection (Databases 2014). It also provides a convenient function to update an existing database with the most recently available data without re-downloading anything. The openWAR package also provides high-level access to Pitch f/x data, but it is currently more limited in the data it can acquire (Baumer, Jensen, and Matthews 2015). It also currently depends on the difficult to install Sxslt package, impeding portability (Temple Lang 2006). openWAR depends on Sxslt to help transform XML files to R data frames via XSL Transformations (XSLT). Without advanced knowledge of XSLT, one must define transformations by hard coding assumptions about the XML format, such as the names of fields of interest. New variables have been added into Pitch f/x several times, and pitchRx automatically picks them up, thanks to functionality provided by XML2R. XML2R makes it easy to wrangle relational data stored as a collection of XML files into a list of data frames. Its interface satisfies principles from pure functional programming: the output of each function can be completely determined from the input. The interface is also predictable: each function inputs and outputs a list of observations (an observation is a matrix with one row). It also represents XML content as a list of observations (matrices with one row), allowing each function to operate on native R data structures, making it more intuitive for R programmers to work with compared to the non-native XMLDocumentContent. This new representation is slightly less computationally efficient in some cases, but it has also made it much easier to implement and maintain higher-level interfaces to specific XML data sources, such as pitchRx and bbscrapeR (Sievert 2014a). To see the fully published article “Taming PITCHf/x Data with XML2R and pitchRx”, see http://rjournal.github.io/archive/2014-1/sievert.pdf References "],
["ldavis-a-method-for-visualizing-and-interpreting-topics.html", "5 LDAvis: A method for visualizing and interpreting topics", " 5 LDAvis: A method for visualizing and interpreting topics The R package LDAvis creates an interactive web-based visualization of a topic model that has been fit to a corpus of text data using Latent Dirichlet Allocation (LDA). Given the estimated parameters of the topic model, it computes various summary statistics as input to a reusable interactive visualization built with HTML, JavaScript, and D3. The goal is to help users interpret the topics in their LDA topic model, and the interactive visualization is primarily useful for quickly viewing, altering, and tracking changes in rankings of terms for a given topic. In a topic model, each topic is defined by a probability mass function over each unique term in the corpus. When studying their differences, analysts often look at lists of the top (say 30) terms of a topic ranked by the estimated probability within that given topic. As discussed in the video below and in our paper, this makes it hard to differentiate meaning between topics since words that are likely to appear overall are also likely to appear in a given topic. Instead, we propose ranking terms using a compromise between this probability and lift (probability within topic divided by overall probability). We also conduct a user study which provides evidence that this compromise helps in identifying topics, and propose a sensible starting point for choosing a compromise; but in practice, users will want to adjust this value and understand how rankings are affected. For this reason, it is important that we assist users in their ability to track changes, by using smooth transitions from one ranking to the next. To read the full paper, see: http://nlp.stanford.edu/events/illvi2014/papers/sievert-illvi2014.pdf "],
["two-new-keywords-for-interactive-animated-plot-design-clickselects-and-showselected.html", "6 Two new keywords for interactive, animated plot design: clickSelects and showSelected", " 6 Two new keywords for interactive, animated plot design: clickSelects and showSelected This paper explains the clickSelects/showSelected paradigm, implemented in animint, which makes it easy to select/query points belonging to arbitrary group(s) and visualize those points in another data space. This differs from the classical linked brushing approach where points must belong to contiguous regions within a subset of the data space. https://github.com/tdhock/animint-paper/blob/master/HOCKING-animint.pdf "],
["designing-and-implementing-an-r-interface-for-interactive-web-graphics.html", "7 Designing and implementing an R interface for interactive web graphics ", " 7 Designing and implementing an R interface for interactive web graphics "],
["introduction.html", "7.1 Introduction", " 7.1 Introduction Point out lack of tools for exploratory web graphics. "],
["case-study.html", "7.2 Case Study", " 7.2 Case Study https://github.com/cpsievert/pedestrians https://github.com/ropenscilabs/eechidna "],
["dynamic-interactive-statistical-web-graphics.html", "7.3 Dynamic interactive statistical web graphics", " 7.3 Dynamic interactive statistical web graphics 7.3.1 Why interactive? Unlike computer graphics which focuses on representing reality, virtually, data visualization is about garnering abstract relationships between multiple variables from visual representation. The dimensionality of data, the number of variables can be anything, usually more than 3D, which summons a need to get beyond 2D canvasses for display. Technology enables this, enabling the user to see many views, query and link components. As demonstrated in Figure using the R package tourbrush (Sievert 2015b), interactive and dynamic graphics allow us to go beyond the constraints of low-dimensional displays to see high-dimensional relationships in data. Dynamic interactive statistical graphics is useful for descriptive statistics, and also to help build better inferential models. Any statistician is familiar with diagnosing a model by plotting data in the model space (e.g., residual plot, qqplot). This works well for determining if the assumptions of a model are adequate, but rarely suggests that our model neglects important features in the data. To combat this problem, (Wickham, Cook, and Hofmann 2015) suggest that we should plot the model in the data space and use dynamic interactive statistical graphics to do so. Interactive graphics have also proved to be useful for exploratory model analysis, a situation where we have many models to evaluate, compare, and critique (Unwin, Volinsky, and Winkler 2003); (Urbanek 2004); (Ripley 2004); (Unwin 2006); (Wickham 2007). With such power comes responsibility that we can verify that visual discoveries are real, and not due to random chance (Buja et al. 2009); (Majumder, Hofmann, and Cook 2013). The ASA Section on Statistical Computing and Graphics maintains a video library which captures many useful dynamic interactive statistical graphics techniques. Several videos show how xgobi (predecessor to ggobi), a dynamic interactive statistical graphics system, can be used to reveal high-dimensional relationships and structures that cannot be easily identified using numerical methods alone (Swayne, Cook, and Buja 1998).2 Another notable video shows how the interactive graphics system mondrian can be used to quickly find interesting patterns in high-dimensional data using exploratory data analysis (EDA) techniques (Theus and Urbanek 2008).3 The most recent video, which shows how dynamic interactive techniques can be used to help interpret a topic model (a statistical mixture model applied to text data) using LDAvis (Sievert and Shirley 2014), is the first web-based visualization in the library, and can be viewed in Figure . In order to be practically useful, interactive statistical graphics must be fast, flexible, accessible, portable, and reproducible. In general, over the last 20-30 years interactive graphics systems were fast and flexible, but were also not easily accessible, portable, or reproducible. The web provides the tool to combat these problems. For example, any visualization created with LDAvis can be shared through a Uniform Resource Locator (URL), meaning that anyone with a web browser and an internet connection can view and interact with a visualization. Furthermore, we can link anyone to any possible state of the visualization by encoding selections with a URL fragment identifier. This makes it possible to link readers to an interesting state of a visualization from an external document, while still allowing them to independently explore the same visualization and assess conclusions drawn from it.4 7.3.2 Indirect versus direct manipulation Even within the statistical graphics community, the term interactive graphics can mean wildly different things to different people (Swayne and Klinke 1999). Some early statistical literature on the topic uses interactive in the sense that an interactive command-line prompt allows users to create graphics on-the-fly (R. A. Becker 1984). That is, users enter commands into the command-line prompt, the prompts evaluates the command, and prints the result (known as the read–eval–print loop (REPL)). Modifying a command to generate another variation of a particular result (e.g., to restyle a static plot) can be thought of as a type of interaction that some might call indirect manipulation. Indirect manipulation can be achieved both from the command-line or from a graphical user interface (GUI). Indirect manipulation from the command-line is more flexible since we have complete control over the commands, but it is also more cumbersome since we must translate our thoughts into code. Indirect manipulation via a GUI is more restrictive, but it helps reduces the the gulf of execution for end-users (i.e., easier to generate desired output) (Hutchins, Hollan, and Norman 1985). In this sense, a GUI can be useful, even for experienced programmers, when the command-line interface impedes our primary task of deriving insight from data. In many cases, the gulf of execution can be further reduced through direct manipulation. Roughly speaking, within the context of interactive graphics, direct manipulation occurs whenever we interact with a plot and reveal new information tied to the event. (Cook and Swayne 2007) use the terms dynamic graphics and direct manipulation to characterize “plots that respond in real time to an analyst’s queries and change dynamically to re-focus, link to information from other sources, and re-organize information.” Perhaps the most powerful direct manipulation technique is the paradigm of linked views (Wilhelm 2005), which will be discussed in more detail in a later section. A simple example to help demonstrate the differences between these interactive techniques would be in an analysis of variance (ANOVA) via multiple boxplots. By default, most plotting libraries sort categories alphabetically, but this is usually not optimal for visual comparison of groups. With a static plotting library such as ggplot2, we could indirectly manipulate the default by going back to the command-line, reordering the factor levels of the categorical variables, and regenerate the plot (Wickham 2009). This is flexible and precise since we may order the levels by any measure we wish (e.g., Median, Mean, IQR, etc.), but it would be much quicker and easier if we had a GUI with a drop-down menu for most of the reasonable sorting options. In a general purpose interactive graphics system such as mondrian, we can use direct manipulation to directly click and drag on the categories to reorder them, making it quick and easy to compare any two groups of interest (Theus and Urbanek 2008). 7.3.3 Linked views and pipelines A general purpose interactive statistical graphics system should possess many direct manipulation techniques such as identifying (i.e., mousing over points to reveal labels), focusing (i.e., view size adjustment, pan and zoom), brushing/identifying, etc. However, it is the intricate management of information across multiple views of data in response to user events that is most valuable. Extending ideas from (Andreas Buja and McDonald 1988), (Wickham et al. 2010) point out that any visualization system with linked views must implement a data pipeline. That is, a “central commander” must be able to handle interaction(s) with a given view, translate its meaning to the data space, and update any linked view(s) accordingly. In order to do so, the commander must know, and be able to compute, function(s) from data to visual space, as well as from visual space to the data. Implementing a pipeline that is fast, general, and able to handle statistical transformations is incredibly difficult. Unfortunately, literature on the implementation of such pipelines is virtually non-existent, but (Xie, Hofmann, and Cheng 2014) provides a nice overview of the implementation details in the R package cranvas (Yihui Xie 2013). 7.3.4 Web graphics Thanks to the constant evolution and eventual adoption of HTML5 as a web standard, the modern web browser now provides a viable platform for building an interactive statistical graphics systems. HTML5 refers to a collection of technologies, each designed to perform a certain task, that work together in order to present content in a web browser. The Document Object Model (DOM) is a convention for managing all of these technologies to enable dynamic and interactive web pages. Among these technologies, there are several that are especially relevant for interactive web graphics: HTML: A markup language for structuring and presenting web content. SVG: A markup language for drawing scalable vector graphics. CSS: A language for specifying styling of web content. JavaScript: A language for manipulating web content. Juggling all of these technologies to just create a simple statistical plot is a tall order. Thankfully, HTML5 technologies are publicly available, and benefit from thriving community of open source developers and volunteers. In the context of web-based visualization, the most influential contribution is Data Driven Documents (D3), a JavaScript library which provides high-level semantics for binding data to web content (e.g., SVG elements) and orchestrating scene updates/transitions (M. B. A. V. O. A. J. Heer 2011). D3 is wildly successful because is builds upon web standards, without abstracting them away, which fosters customization and interoperability. However, compared to a statistical graphics environments like R, creating basic charts is complicated, and a large amount of code must be hard-wired to each visualization. Fortunately, there are a number of ways to provide higher-level interfaces to web graphics, and we focus on R interfaces. 7.3.5 Translating R graphics to the web There are a few ways to simply translate R graphics to a web format, such as SVG. R has built-in support for a SVG graphics device, made available through the svg() function, but it can be quite slow, which inspired the new svglite package (Wickham et al., n.d.). The SVGAnnotation package provides some functionality to post-process SVG files generated with svg() to add some basic interactivity and animation (Nolan and Temple Lang 2012). The gridSVG package is specially designed to translate grid graphics (e.g., ggplot2, lattice, etc.) to SVG, and preserves the naming information of grid objects, making it easier to layer on interactive functionality (Potter and Murrell 2012). (Fujino 2015) uses gridSVG to enable linked brushing between ggplot2 graphics, but only implements a few chart types. (Riutta et. al. and Russell 2015) uses gridSVG to provide pan and zoom capability to virtually any R graphic. The animint and plotly packages take a different approach to translating ggplot2 graphics to a web format (Hocking, VanderPlas, and Sievert 2015); (Sievert et al., n.d.). Instead of translating directly to SVG via gridSVG, they extract relevant information from the internal representation of a ggplot2 graphic5, store it in JavaScript Object Notation (JSON), and pass the JSON as input to a JavaScript function, which then produces a web based visualization. It is becoming more and more popular to see JavaScript graphing libraries use this design pattern (sometimes referred to as a JSON specification or schema), since it separates out what is information contained in the the graphic from how to actually draw it. This has a number of advantages; for example, plotly graphics can be rendered in SVG, or using WebGL (based on HTML5 canvas, not SVG) which allows the browser to render many more graphical marks by leveraging the GPU. Converting static graphics to web formats such as SVG or canvas not only allows us to embed the graphics into larger HTML documents, but it also allows us to inject basic interactive features at no cost to the user. For example, in animint and plotly we provide tool-tips and clickable legends that show/hide graphical marks corresponding to the legend entry. In the case of animint, we have also extended ggplot2 grammar of graphics to enable animations and categorical linking between plots with relatively small amount of effort by users. This extension is discussed at length in Two new keywords for interactive, animated plot design: clickSelects and showSelected 7.3.6 R interfaces for interactive web graphics Translating existing graphics to a web-based format is useful for quickly breathing new life into existing code, but it is fairly limited in how far we can take it. Assuming the goal is to have a general, yet high-level, interface for creating highly dynamic interactive web graphics from R, we’re better off building a new interface designed exactly for this purpose. The first serious attempt in this direction was the R package rCharts, whose R interface is heavily inspired by lattice (Vaidyanathan 2013). The most impressive result of rCharts’s design is its ability to interface with many different JavaScript charting libraries. However, rCharts has little to no support for coordination of dynamic linked views from R. Another notable interface for creating interactive web graphics from R is ggvis, a reworking of ggplot2’s grammar of graphics to incorporate interactivity (Chang and Wickham 2015). Similar to animint, ggvis encodes plot specific information as JSON, but instead of writing a JavaScript renderer from the ground up, it uses Vega, a popular JSON schema for creating web-based graphics (Heer 2014). This limits the flexibility of ggvis, but it also drastically reduces the overhead in maintaining such a software project, allowing the focus to be on building a grammar for expressing interactions from R. The current version of ggvis uses an old version of vega, before support for reactive components was added to its JSON schema. To recognize user interactions with vega graphics, ggvis has its own custom JavaScript designed specifically for vega. To enable support for coordinated linked views, it exposes the data pipeline to users via the R package shiny, a framework for writing web applications in R. A web application is a website which, when visited by users (aka clients), communicates with a web server. This approach is useful when a website needs to execute code that can not be executed in the web browser (e.g., R code). Figure provides a visual demonstration of this model and its relation to the data pipeline necessary for coordinating linked views. Generally speaking, websites that render entirely client-side are more desirable since they are easier to share, more responsive, and require less computational resources to run6. However, the client-server approach can be very useful for dynamically performing statistical computations, an ability that commonly seen in interactive statistical graphics. (Urbanek and Horner 2015) and (Jeroen Ooms 2014b) also allow us to execute R code on a web server, and retrieve output via HTTP, but shiny is the most heavily used since apps can be written entirely in R using a very powerful, yet approachable, reactive programming framework for handling user events. There are also many convenient shortcuts for creating attractive HTML input forms, making it incredibly easy to go from R script to an web app powered by R that dynamically updates when users alter input values. In other words, shiny makes it quick and easy to write web-based GUIs with support for indirect manipulation. Historically, an advanced understanding of shiny and JavaScript was required to implement direct manipulation in a shiny app. Recently, shiny added support for retrieving information on user events with static R graphics7, allowing developers to coordinate views in a web app with no JavaScript involved. This is a powerful tool for R users, but it has its weaknesses. Most importantly, its not clear how to handle interactions when positional scales are categorical (e.g., a bar chart) or how to provide a visual clue for the selection. The touring video in Figure purposefully uses shiny’s built-in support for brushing to demonstrate the problem with providing a visual clue. This points to the fundamental problem in using non-web-based graphics to implement interactive graphics in a web browser: every time the view updates, the entire frame must be regenerated, resulting in a “glitch” effect. If the plot being brushed used native web graphics (e.g., SVG), it would allow for finer control over how the view updates in response to user interactions and/or dynamic data. On the other hand, since ggvis is web-based, and has special client-side functionality, it knows how to smoothly transition from one frame to the next when provided with new data from the shiny server, which is crucial for constructing a mental model of the data space. Having richer interfaces for generating web-based interactive graphics from R that can share selections, and handle smooth transitions, would make this, and many other examples, generally better. (source code here -- ) --> Many web-based graphing toolkits have appeared since the advent of rCharts, making a single package that interfaces with every toolkit infeasible. Some ideas deriving from work on rCharts, such as providing the glue to render plots in various contexts (e.g., the R console, shiny apps, and rmarkdown documents), have evolved into the R package htmlwidgets (Vaidyanathan et al. 2015). Having built similar bridges for animint and LDAvis, I personally know and appreciate the amount of time and effort this package saves other package authors. The htmlwidgets framework is not constrained to just graphics, it simply provides a set of conventions for authoring web content from R. Numerous JavaScript data visualization libraries are now made available using this framework, most designed for particular use cases, such as leaflet for geo-spatial mapping, dygraphs for time-series, and networkD3 for networks (Cheng and Xie 2015); (Vanderkam and Allaire 2015); (Gandrud, Allaire, and Russell 2015).8 There are also HTML widgets that provide an interface to more general purpose visualization JavaScript libraries such as plotly, rbokeh, and rcdimple (Sievert et al., n.d.); (Hafen and team 2015); (Kiernander et al. 2015). Most of these JavaScript libraries provide at least some native support for direct manipulation such as identifying (i.e., mousing over points to reveal labels), focusing (i.e., pan and zoom), and sometimes highlighting (i.e., brushing over points to highlight points in another view). More often than not, the support for dynamic and linked views is lacking, especially if we want to define the linking in R, and produce a standalone HTML document. The R package crosstalk is a new framework for coordinating arbitrary HTML widgets (Cheng 2015a). It provides both an R and a JavaScript API for querying selections, meaning crosstalk powered HTML widgets can work with or without shiny, and if implemented carefully by HTML widget authors, provides a means for coordinating multiple HTML widgets without shiny. At it’s core, crosstalk only provides a standard way to set, store, and access selection values, so the actual logic for updating views based on the selection value(s) is on the HTML widget author, and this part is far from trivial. The first HTML widget to leverage crosstalk was (Cheng 2015b), but is limited to linked brushing on scatterplots. it is not yet clear how to implement pipelines where the function between the data and visual marks something other than the identity function. plotly also has some support for sharing click events9, with support for brush events coming soon. Having HTML widgets that can share selections with each other will be a huge step forward for web-based interactive graphics. With some effort and careful implementation by HTML widget authors, it may be possible to provide sensible defaults for updating views between arbitrary widgets, but users that know some JavaScript should be able change these defaults from R. In time, the htmlwidget package will provide conventions for this, by allowing one to send arbitrary JavaScript functions from R that execute after the widget has rendered in the browser. The biggest problem in implementing coordinated widgets will be in managing data structures, since each widget will likely have its own data structure for representing a selection. In this case, in order to coordinate them, users may have to embed widgets in a shiny app to access and organize selections. This gives users tremendous control over sharing selections, but may limit control over smooth transitions between states of a given widget, a quality that is essential for dynamic graphics. * How to handle multiple, concurrent users? > opencpu and FastRWeb enjoy better overall performance compared to shiny since R sessions are stateless. * What is missing is something akin to the mutaframe (__mutatr__?), that can work entirely client-side (inside the browser), but can also easily integrate with an R server framework (e.g. __shiny__). --> % transform(rate = unemploy / pop) %>% plot_ly(x = date, y = rate, name = \"raw\") %>% loess(rate ~ as.numeric(date), data = .) %>% augment() %>% add_trace(y = .fitted, name = \"smooth\") %>% layout(title = \"Proportion of U.S. population that is unemployed\") ``` ![](plotly.png) To make this possible, a special environment within __plotly__'s namespace tracks not only visual mappings/properties, but also the order in which they are specified. So, if a __plotly__ function used to modify a visualization (e.g., `add_trace()` or `layout()`) receives a data frame without any special attributes, it retrieves the last plot created, and modifies that plot. __animint__ and __plotly__ could be classified as general purpose software for web-based interactive and dynamic statistical graphics; whereas __LDAvis__, could be classified as software for solving a domain specific problem. The __LDAvis__ package creates an interactive web-based visualization of a topic model fit to a corpus of text data using Latent Dirichlet Allocation (LDA) to assist in interpretation of topics. The visualization itself is written entirely with HTML5 technologies and makes use of the `JavaScript` library d3js [@Bostock:2011] to implement advanced interaction techniques that higher-level tools such as __plotly__, __animint__, and/or __shiny__ do not currently support. --> https://gallery.shinyapps.io/LDAelife * Helped implement the completely client-side application -> https://cpsievert.github.io/LDAvis/reviews/vis/ --> --> References "],
["testing-html-widgets-from-r.html", "8 Testing HTML widgets from R", " 8 Testing HTML widgets from R The current trend in testing HTML widget R packages is to verify that certain R commands construct an expected data structure. However, since this data structure has to be converted to JSON, and input into a JavaScript function(s), this approach doesn’t guarantee that the end result is correct. Some HTML widgets use JavaScript libraries with their own testing framework, but if those tests change, R package authors may be oblivious to errors when upgrading to a new version. A proper testing framework for this type of software should be able to programmatically ensure that R commands create the correct web content, which involves constructing, parsing, and possibly manipulating the Document Object Model (DOM). HTML widgets that don’t change based on user events will only have to construct and parse the DOM, so an R package like rdom would be suitable for implementing tests. HTML widgets that do change based on user events will need to program these events using something like RSelenium. I know first-hand from work on animint that integrating RSelenium with a unit testing framework such as testthat is not trivial, so the community could benefit from a detailed explanation of the approach. "]
]
