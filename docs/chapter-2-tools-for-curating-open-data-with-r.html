<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Interfacing R with Web Technologies for Data Acquistion and Interactive Visualization</title>
  <meta content="text/html; charset=UTF-8" http-equiv="Content-Type">
  <meta name="description" content="An overview of the R package plotly">
  <meta name="generator" content="bookdown 0.1 and GitBook 2.6.7">

  <meta property="og:title" content="Interfacing R with Web Technologies for Data Acquistion and Interactive Visualization" />
  <meta property="og:type" content="book" />
  <meta property="og:url" content="<a href="http://cpsievert.github.io/phd-thesis" class="uri">http://cpsievert.github.io/phd-thesis</a>" />
  
  <meta property="og:description" content="An overview of the R package plotly" />
  <meta name="github-repo" content="cpsievert/phd-thesis" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Interfacing R with Web Technologies for Data Acquistion and Interactive Visualization" />
  
  <meta name="twitter:description" content="An overview of the R package plotly" />
  

<meta name="author" content="Carson Sievert">

<meta name="date" content="2016-08-24">


  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="curating-open-data-in-r.html">
<link rel="next" href="scraping-dynamic-web-pages.html">

<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />









<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

<link rel="stylesheet" href="css/style.css" type="text/css" />
</head>

<body>


  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">PhD Thesis</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Abstract</a><ul>
<li class="chapter" data-level="0.1" data-path="what-makes-a-good-software-interface.html"><a href="what-makes-a-good-software-interface.html"><i class="fa fa-check"></i><b>0.1</b> What makes a good software interface?</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="acquiring-and-wrangling-web-content-in-r.html"><a href="acquiring-and-wrangling-web-content-in-r.html"><i class="fa fa-check"></i><b>1</b> Acquiring and wrangling web content in R</a><ul>
<li class="chapter" data-level="1.1" data-path="interfaces-for-working-with-web-content.html"><a href="interfaces-for-working-with-web-content.html"><i class="fa fa-check"></i><b>1.1</b> Interfaces for working with web content</a></li>
<li class="chapter" data-level="1.2" data-path="interfaces-for-acquiring-data-on-the-web.html"><a href="interfaces-for-acquiring-data-on-the-web.html"><i class="fa fa-check"></i><b>1.2</b> Interfaces for acquiring data on the web</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="curating-open-data-in-r.html"><a href="curating-open-data-in-r.html"><i class="fa fa-check"></i><b>2</b> Curating Open Data in R</a><ul>
<li class="chapter" data-level="2.0.1" data-path="curating-open-data-in-r.html"><a href="curating-open-data-in-r.html#the-rise-of-open-data"><i class="fa fa-check"></i><b>2.0.1</b> The Rise of Open Data</a></li>
<li class="chapter" data-level="2.0.2" data-path="curating-open-data-in-r.html"><a href="curating-open-data-in-r.html#what-makes-data-on-the-web-open"><i class="fa fa-check"></i><b>2.0.2</b> What Makes Data on the Web “Open”?</a></li>
<li class="chapter" data-level="2.0.3" data-path="curating-open-data-in-r.html"><a href="curating-open-data-in-r.html#on-the-quality-quantity-and-accessibility-of-open-data"><i class="fa fa-check"></i><b>2.0.3</b> On the Quality, Quantity, and Accessibility of Open Data</a></li>
<li class="chapter" data-level="2.0.4" data-path="curating-open-data-in-r.html"><a href="curating-open-data-in-r.html#best-practices-for-publishing-open-data"><i class="fa fa-check"></i><b>2.0.4</b> Best practices for publishing open data</a></li>
<li class="chapter" data-level="2.0.5" data-path="curating-open-data-in-r.html"><a href="curating-open-data-in-r.html#preserving-open-data"><i class="fa fa-check"></i><b>2.0.5</b> Preserving Open Data</a></li>
<li class="chapter" data-level="2.0.6" data-path="curating-open-data-in-r.html"><a href="curating-open-data-in-r.html#r-as-a-data-curation-engine"><i class="fa fa-check"></i><b>2.0.6</b> R as a Data Curation Engine</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="chapter-2-tools-for-curating-open-data-with-r.html"><a href="chapter-2-tools-for-curating-open-data-with-r.html"><i class="fa fa-check"></i><b>3</b> Chapter 2: Tools for Curating Open Data with R</a><ul>
<li class="chapter" data-level="3.1" data-path="scraping-dynamic-web-pages.html"><a href="scraping-dynamic-web-pages.html"><i class="fa fa-check"></i><b>3.1</b> Scraping Dynamic Web Pages</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="taming-pitchfx-data-with-xml2r-and-pitchrx.html"><a href="taming-pitchfx-data-with-xml2r-and-pitchrx.html"><i class="fa fa-check"></i><b>4</b> Taming PITCHf/x Data with XML2R and pitchRx</a></li>
<li class="chapter" data-level="5" data-path="ldavis-a-method-for-visualizing-and-interpreting-topics.html"><a href="ldavis-a-method-for-visualizing-and-interpreting-topics.html"><i class="fa fa-check"></i><b>5</b> LDAvis: A method for visualizing and interpreting topics</a></li>
<li class="chapter" data-level="6" data-path="two-new-keywords-for-interactive-animated-plot-design-clickselects-and-showselected.html"><a href="two-new-keywords-for-interactive-animated-plot-design-clickselects-and-showselected.html"><i class="fa fa-check"></i><b>6</b> Two new keywords for interactive, animated plot design: clickSelects and showSelected</a></li>
<li class="chapter" data-level="7" data-path="designing-and-implementing-an-r-interface-for-interactive-web-graphics.html"><a href="designing-and-implementing-an-r-interface-for-interactive-web-graphics.html"><i class="fa fa-check"></i><b>7</b> Designing and implementing an R interface for interactive web graphics</a><ul>
<li class="chapter" data-level="7.1" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i><b>7.1</b> Introduction</a></li>
<li class="chapter" data-level="7.2" data-path="case-study.html"><a href="case-study.html"><i class="fa fa-check"></i><b>7.2</b> Case Study</a></li>
<li class="chapter" data-level="7.3" data-path="dynamic-interactive-statistical-web-graphics.html"><a href="dynamic-interactive-statistical-web-graphics.html"><i class="fa fa-check"></i><b>7.3</b> Dynamic interactive statistical web graphics</a><ul>
<li class="chapter" data-level="7.3.1" data-path="dynamic-interactive-statistical-web-graphics.html"><a href="dynamic-interactive-statistical-web-graphics.html#why-interactive"><i class="fa fa-check"></i><b>7.3.1</b> Why interactive?</a></li>
<li class="chapter" data-level="7.3.2" data-path="dynamic-interactive-statistical-web-graphics.html"><a href="dynamic-interactive-statistical-web-graphics.html#indirect-versus-direct-manipulation"><i class="fa fa-check"></i><b>7.3.2</b> Indirect versus direct manipulation</a></li>
<li class="chapter" data-level="7.3.3" data-path="dynamic-interactive-statistical-web-graphics.html"><a href="dynamic-interactive-statistical-web-graphics.html#linked-views-and-pipelines"><i class="fa fa-check"></i><b>7.3.3</b> Linked views and pipelines</a></li>
<li class="chapter" data-level="7.3.4" data-path="dynamic-interactive-statistical-web-graphics.html"><a href="dynamic-interactive-statistical-web-graphics.html#web-graphics"><i class="fa fa-check"></i><b>7.3.4</b> Web graphics</a></li>
<li class="chapter" data-level="7.3.5" data-path="dynamic-interactive-statistical-web-graphics.html"><a href="dynamic-interactive-statistical-web-graphics.html#translating-r-graphics-to-the-web"><i class="fa fa-check"></i><b>7.3.5</b> Translating R graphics to the web</a></li>
<li class="chapter" data-level="7.3.6" data-path="dynamic-interactive-statistical-web-graphics.html"><a href="dynamic-interactive-statistical-web-graphics.html#r-interfaces-for-interactive-web-graphics"><i class="fa fa-check"></i><b>7.3.6</b> R interfaces for interactive web graphics</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="testing-html-widgets-from-r.html"><a href="testing-html-widgets-from-r.html"><i class="fa fa-check"></i><b>8</b> Testing HTML widgets from R</a></li>
<li class="divider"></li>
<li><a href="https://bookdown.org" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Interfacing R with Web Technologies for Data Acquistion and Interactive Visualization</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="chapter-2-tools-for-curating-open-data-with-r" class="section level1">
<h1><span class="header-section-number">3</span> Chapter 2: Tools for Curating Open Data with R</h1>
<p>If data isn’t conveniently accessible as a tabular text file (such as csv or tsv), working with open data typically requires some knowledge of a constantly evolving set of network protocols and Web technologies. This presents a barrier to access for many researchers, but there is a large effort to lower the barrier, especially for the R project. The Web Technologies and Service CRAN Task View does a great job listing all of these efforts for the R project <span class="citation">(Chamberlain et al. <a href="#ref-web-task-view">2015</a>)</span>. Most of these tools can be grouped into one of three categories:</p>
<ol style="list-style-type: decimal">
<li><p>A high level interface with direct access to data ready for statistical analysis. In this case, the interface is typically restricted to a single data source, but users can obtain tidy data <span class="citation">(Wickham <a href="#ref-tidy-data">2014</a>)</span> without any knowledge of protocols or Web Technologies. Some interfaces, such as <strong>pitchRx</strong> <span class="citation">(Sievert <a href="#ref-Sievert:2014a">2014</a><a href="#ref-Sievert:2014a">b</a>)</span>, perform all the steps of data curation under the hood and require a tremendous amount of work by the author. Other interfaces may simply wrap an existing web API for accessing already tidy data.</p></li>
<li><p>A grammar for transforming non-tidy information into a tidy form. In this case, the tool is typically restricted to a specific file format such as HTML, XML or JSON. However, in some cases, it can remove any requirements/skills required for transforming these file formats. For example, <strong>XML2R</strong> <span class="citation">(Sievert <a href="#ref-Sievert:2014a">2014</a><a href="#ref-Sievert:2014a">b</a>)</span> makes it possible to transform XML into a tidy form without XPATH and can make it easier build and maintain interfaces that fall under (1) (such as <strong>pitchRx</strong> and <strong>bbscrapeR</strong>).</p></li>
<li><p>A low level interface for working with network protocols and Web Technologies. Using these interfaces require an understanding of popular network protocols such as HTTP/HTTPS, data formats such as JSON and XML, and Web technologies such as HTML/JavaScript/etc. Tools that fall under (1) and (2) build on top of tools under (3).</p></li>
</ol>
<p>This write-up focuses on how to build tools under (1) and (2) using tools under (3)?</p>
<p>Some of the lower-level tools require knowledge of technologies such as HTTP</p>
<p>A couple projects, such as <strong>XML2R</strong> <span class="citation">(Sievert <a href="#ref-Sievert:2014a">2014</a><a href="#ref-Sievert:2014a">b</a>)</span> or <strong>tidyjson</strong> (needs citation) provide a high-level grammar for transforming . A more common, but less generic, approach is to provide a direct interface to specific data source(s) – one such example is . In some cases, this approach isn’t ideal, since it may require assumptions about the analysis to be performed.</p>
<p>Better tools might also encourage statisticians to become more hands-on during the acquisition, transformation, and cleaning of data. This is important for reasons similar to why it’s important to involve a statistician in the design of an experiment. A number of decisions made during data curation can effect data quality. In fact, it may be that a very large portion of the available data is bad, irrelevant to the analysis, and/or simply too large to preserve in bulk. Having a solid foundation in statistical modeling certainly helps in resolving these issues and can lead to more accurate conclusions.</p>
<div id="packaging-access-to-specific-data-sources" class="section level4">
<h4><span class="header-section-number">3.0.0.1</span> Packaging Access to Specific Data Sources</h4>
<p><code>R</code> users are probably familiar with the <strong>datasets</strong> package (distributed with R itself) which provides direct access to classic datasets such as <code>iris</code> or <code>mtcars</code>. <code>R</code>’s packaging mechanism provides a very convenient way to distribute datasets, but it does come with some limitations, which presents problems for packaging open data sources:</p>
<ol style="list-style-type: decimal">
<li>CRAN imposes size restrictions on the size of the package.</li>
<li>CRAN imposes limitations on the frequency of package updates.</li>
<li>Some open data sources have terms of use which include restrictions on redistributing the data itself.</li>
</ol>
<p>To avoid these problems, we’ve seen a number of <code>R</code> packages which provide function(s) to extract open data from it’s source, clean and transform it into a usable form. One example of this approach is the <strong>pitchRx</strong> package where users just have to provide a range of dates to acquire data:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(pitchRx)
dat &lt;-<span class="st"> </span><span class="kw">scrape</span>(<span class="dt">start =</span> <span class="st">&quot;2008-01-01&quot;</span>, <span class="dt">end =</span> <span class="kw">Sys.Date</span>())</code></pre></div>
<p>Since the data source contains a number of observational units, the <code>scrape()</code> function returns a list of data frames (one data frame for each unit). These can easily be exported as database tables using a database of the users choice via R’s database interface <span class="citation">(Databases <a href="#ref-DBI">2014</a>)</span>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># start a database connection</span>
con &lt;-<span class="st"> </span>DBI::<span class="kw">dbConnect</span>(RSQLite::<span class="kw">SQLite</span>(), <span class="st">&quot;mydb.sqlite3&quot;</span>)
<span class="co"># extract, transform, and load into a database</span>
<span class="kw">scrape</span>(<span class="dt">start =</span> <span class="st">&quot;2008-01-01&quot;</span>, <span class="dt">end =</span> <span class="kw">Sys.Date</span>(), <span class="dt">con =</span> con)</code></pre></div>
<p>It’s worth noting that the three main jobs of the <code>scrape()</code> function is to extract, transform, and (optionally) load data into a database. These are three general operations that similar packages probably want to perform. Although the extract and transform steps will vary dramatically from package to package, the <strong>etl</strong> package provides a common interface to promote consistency across packages <span class="citation">(Baumer and Sievert, n.d.)</span>.</p>
</div>
<div id="extract-transform-and-load-paradigm" class="section level4">
<h4><span class="header-section-number">3.0.0.2</span> Extract, Transform, and Load Paradigm</h4>
<blockquote>
<p>It is all too easy for statistical thinking to get swamped by programming tasks. <span class="citation">(Ripley and Ripley <a href="#ref-Ripley:2001">2001</a>)</span></p>
</blockquote>
<p><strong>etl</strong> is to <strong>bbscrapeR</strong> as <strong>DBI</strong> is to <strong>RSQLite</strong> in the sense that <strong>etl</strong> and <strong>DBI</strong> provide a set of generic functions while <strong>bbscrapeR</strong> and <strong>RSQLite</strong> implement methods specific to an application. In other words, <strong>bbscrapeR</strong> implements methods for <strong>etl</strong> generics that extract, transform, and load a <em>specific</em> (basketball) data source and <strong>RSQLite</strong> implements methods for <strong>DBI</strong> generics that allow one to communicate with a MySQL database from <code>R</code>.</p>
</div>
<div id="working-with-popular-web-data-formats" class="section level4">
<h4><span class="header-section-number">3.0.0.3</span> Working with Popular Web Data Formats</h4>
<p>Although many formats exist, the majority of data transferred over the web comes in two forms: XML and JSON (needs citation). These formats are designed to be machine readable. That is, given a set of directions, a computer can store and parse information within these files. These formats are great for use in web applications where machines communicate and transfer data. In many cases, this data has valuable information that <em>humans</em> want to analyze. Before that can happen, however; analysts typically have to request and transform HTTP responses into forms suitable for analysis and/or visualization.</p>
</div>
<div id="requesting-web-content-sec-request" class="section level4">
<h4><span class="header-section-number">3.0.0.4</span> Requesting Web Content {#sec: request}</h4>
<p>Before transforming XML/JSON data structures, one typically requests content from a web server via a communication protocol. The most ubiquitous protocol is the Hypertext Transfer Protocol (HTTP). Base R has built-in utilities for reading data (or <code>GET</code>ting) via HTTP, but this use case is quite limited. For example, if the analyst wishes to obtain data over a secure connection, such as HTTP Secure (HTTPS), other methods must be used.</p>
<p>cURL is a widely used command line tool which covers many protocols for transferring data between machines. The RCurl package provides a low-level interface to cURL with some additional tools for processing <span class="citation">(Temple Lang <a href="#ref-RCurl">2014</a><a href="#ref-RCurl">a</a>)</span></p>
<p>for transferring data between machines using Uniform Resource Locators (URLs) and .</p>
<ul>
<li>RCurl</li>
<li>httr</li>
<li>does XML/rvest fit here?</li>
</ul>
</div>
<div id="transforming-web-content-into-structured-data-sec-transform" class="section level4">
<h4><span class="header-section-number">3.0.0.5</span> Transforming Web Content into Structured Data {#sec: transform}</h4>
<p>Working directly with XML/JSON presents challenges for data analysis and statistical modeling. The XML/JSON specifications allow for deeply nested and non-relational data structures; however, popular statistical computing software assumes data exists in a tabular format – each row represents the observational unit and each column represents attributes associated with each observation <span class="citation">(Wickham <a href="#ref-tidy-data">2014</a>)</span>. A number of efforts exist for working with XML/JSON, but in many cases, there is no standard or well-defined way to transform these data structures into a tabular/tidy format. As a result, the analyst is left to handle reshaping of the data into a usable format for data analysis.</p>
<ul>
<li>introduce tidy data framework?</li>
<li>XML2R</li>
<li>Pros: Easy to <em>express</em> how to go from “unstructured” XML to tidy data.</li>
<li>Cons: Must be able to pull XML into memory (consequence of the implementation, not of the concept itself)</li>
<li>Future work: C++ backend to do dplyr-esque lazy computations? (TODO: look at connecting to a XML database)</li>
</ul>
</div>
<div id="cleaning-data-for-statistical-analysis" class="section level4">
<h4><span class="header-section-number">3.0.0.6</span> Cleaning data for statistical analysis</h4>
<ul>
<li><a href="https://github.com/data-cleaning/editrules" class="uri">https://github.com/data-cleaning/editrules</a></li>
<li><a href="https://github.com/data-cleaning/validate" class="uri">https://github.com/data-cleaning/validate</a></li>
</ul>
</div>
<div id="working-with-databases" class="section level4">
<h4><span class="header-section-number">3.0.0.7</span> Working with databases</h4>
<ul>
<li>DBI</li>
<li>dplyr</li>
</ul>
<p>==============================================</p>
<p>The <a href="https://github.com/ropensci/opendata">CRAN task view on Open Data</a> does a great job of summarizing of <code>R</code> clients which connect to these and other open data efforts.</p>
<p>Prerequisites to performing analysis on this data might require the discovery, parsing, cleaning, transformation, and integration of data with other sources. <!-- A paragraph on large scale attempts to solve this?
  * Proposed standards such as the Semantic Web
  * Automated systems for searching/downloading curated data?
--></p>
<p>A number of disciplines have produced substantial works aimed at simplifying various stages of the data curation process. Although some tasks can be automated or guided by clever systems, at some level, human intervention is required during the curation process. Curation typically requires a custom set of instructions for extracting and structuring information for downstream analysis. The domain knowledge required to write these instructions varies wildly depending on the application.</p>
<p>The contribution of this work is an overview of modern tools for curating data from the Web using R. An emphasis is placed on problems that data-driven researchers are likely to face and solutions that minimize the amount of prior knowledge and cognitive effort required. The goal is provide a taxonomy of practical problems and solutions that the community finds useful.</p>
</div> 
</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-web-task-view">
<p>Chamberlain, Scott, Thomas Leeper, Patrick Mair, Karthik Ram, and Christopher Gandrud. 2015. “CRAN Task View: Web Technologies and Services.” <a href="http://cran.r-project.org/web/views/WebTechnologies.html" class="uri">http://cran.r-project.org/web/views/WebTechnologies.html</a>.</p>
</div>
<div id="ref-tidy-data">
<p>Wickham, Hadley. 2014. “Tidy Data.” <em>The Journal of Statistical Software</em> 59 (10). <a href="http://www.jstatsoft.org/v59/i10/" class="uri">http://www.jstatsoft.org/v59/i10/</a>.</p>
</div>
<div id="ref-Sievert:2014a">
<p>Sievert, Carson. 2014b. “Taming Pitchf/X Data with pitchRx and XML2R.” <em>The R Journal</em> 6 (1). <a href="http://journal.r-project.org/archive/2014-1/sievert.pdf" class="uri">http://journal.r-project.org/archive/2014-1/sievert.pdf</a>.</p>
</div>
<div id="ref-DBI">
<p>Databases, R Special Interest Group on. 2014. <em>DBI: R Database Interface</em>. <a href="http://CRAN.R-project.org/package=DBI" class="uri">http://CRAN.R-project.org/package=DBI</a>.</p>
</div>
<div id="ref-Ripley:2001">
<p>Ripley, B D, and R M Ripley. 2001. “Applications of R Clients and Servers.” <em>Proceedings of the Nd International Workshop on Distributed Statistic Al Computing</em>, March, 1–7.</p>
</div>
<div id="ref-RCurl">
<p>Temple Lang, Duncan. 2014a. <em>RCurl: General Network (Http/Ftp/.) Client Interface for R</em>. <a href="http://CRAN.R-project.org/package=RCurl" class="uri">http://CRAN.R-project.org/package=RCurl</a>.</p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="curating-open-data-in-r.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="scraping-dynamic-web-pages.html" class="navigation navigation-next " aria-label="Next page""><i class="fa fa-angle-right"></i></a>

<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"facebook": true,
"twitter": true,
"google": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/cpsievert/phd-thesis/edit/gh-pages/01-web-scraping.Rmd",
"text": "Edit"
},
"download": ["bookdown.pdf", "bookdown.epub", "bookdown.mobi"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

</body>

</html>
