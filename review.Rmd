---
title: "Leveraging Web Technologies for Data Science."
author: "Carson Sievert"
output: pdf_document
bibliography: review.bib
---

# Alt Titles

Mind the gap: leveraging, connecting and extending R's ecosystem for exploratory data analysis
Mind the gap: Interfacing Statitstical Software with Modern Web Technologies
Connect the dots: Documenting and Enabling Data Science Workflows

# Preface

The World Wide Web has fundamentally changed the way content is created, shared, and consumed. There is no question it has profoundly impacted the way we perform, present, and think about data analysis. As [@nolan-lang] states:

> "[The Web] has helped broaden the focus of statistics from the modeling stage to all stages of data science: finding relevant data, accessing data, reading and transforming data, visualizing the data in rich ways, modeling, and presenting the results and conclusions with compelling, interactive displays."

This shift in focus for statistics has gained momentum in recent years as improvements in Web technologies have spurred innovation and reduced barriers to access. A number of advances have reduced the cognitive load traditionally required in Web programming and enabled new cohorts of scientists to create, discover, and share their findings in novel ways without significant start-up costs. For example, without leaving their respective languages, `R` and `python` users can: create Web applications/documents, acquire & manipulate Web documents, perform scraping/crawling/headless-browsing tasks, communicate with Web servers, and create interactive visualizations. As a result, statisticians are experiencing greater opportunity to impact all stages of the data science workflow.

A particularly interesting stage, which does not receive a lot of attention, is the interplay between interactive visualization and modeling. Web documents are inherently interactive, but `JavaScript` is limited in terms of access to statistical algorithms & scientific computing resources. In this regard, enabling Web documents to use facilities available to other languages is a promising platform for exploratory data analysis (EDA) and interactive model visualization. EDA software certaintly existed before the Web, but the modern browser as a visualization platform helps increase visibility, ease of use, and attention.

The following sections provide an overview of concepts and implementations of statistical software aimed at various stages of the data science workflow. Since modeling is very, very large area, we cover a only a few types of models that can scale to large datasets.  

# Requesting and Transforming Data on the Web for Statistical Analysis

Although many formats exist, the majority of data transferred over the web comes in two forms: XML and JSON (needs citation). These formats are designed to be machine readable. That is, given a set of directions, a computer can store and parse information within these files. These formats are great for use in web applications where machines communicate and transfer data. In many cases, this data has valuable information that *humans* want to analyze. Before that can happen, however; analysts typically have to request and transform HTTP responses into forms suitable for analysis and/or visualization.

## Making HTTP requests {#sec: http}

Before transforming and/or reshaping these data structures, one typically requests it from a web server. 

* RCurl
* httr
* XML?

## Transforming Web Content into Structured Data

Working directly with XML/JSON presents challenges for data analysis and statistical modeling. The XML/JSON specifications allow for deeply nested and non-relational data structures; however, popular statistical computing software assumes data exists in a tabular format -- each row represents the observational unit and each column represents attributes associated with each observation [@tidy-data]. A number of efforts exist for working with XML/JSON, but in many cases, there is no standard or well-defined way to transform these data structures into a tabular/tidy format. As a result, the analyst is left to handle reshaping of the data into a usable format for data analysis.

* does rvest fit here?
* introduce tidy data framework?
* XML2R
  * Pros: Easy to _express_ how to go from "unstructured" XML to tidy data.
  * Cons: Must be able to pull XML into memory (consequence of the implementation, not of the concept itself)
  * Future work: C++ backend to do dplyr-esque lazy computations? (TODO: look at connecting to a XML database)

## High-level APIs for Acquiring Structured Data off the Web

* pitchRx
* bbscrapeR

# Interactive Web Documents

* RServe
* FastRWeb
* opencpu
* shiny

# Web-based Interactive Statistical Graphics for Data Analysis

## History of Interactive Statistical Graphics

Over the course of several decades, many advances in the area of dynamic and interactive statistical graphics were made. Most of this work was done before the web-browser became a promising platform for interactive graphics. 

## Modern Web-based Interactive Statistical Graphics

* D3js
* Vega
* ggvis
* htmlwidgets
* rCharts
* animint
* Tableau
* LDAvis

### shiny versus animint

* The main benefit of having an R Web server back-end for Web-based visualizations is the ability to performs statistical computations on-the-fly. This is typically needed when 

of the data space where 

large amounts of rendering is involved. A high performance 