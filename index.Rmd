---
title: "Interfacing R with Web Technologies for Interactive Statistical Graphics and Computing with Data"
author: Carson Sievert
documentclass: isuthesis
fontsize: 12pt
bibliography: references.bib
site: bookdown::bookdown_site
url: 'http://cpsievert.github.io/phd-thesis'
github-repo: cpsievert/phd-thesis
always_allow_html: yes
---

```{r, include=FALSE}
options(
  dplyr.print_min = 6, dplyr.print_max = 6, 
  digits = 3, htmltools.dir.version = FALSE
)
knitr::opts_chunk$set(
  comment = "#>",
  collapse = TRUE,
  warning = FALSE,
  message = FALSE,
  cache = TRUE,
  # decrease size of browser window and wait 2 seconds before taking screenshot
  screenshot.opts = list(vwidth = 500, vheight = 300, delay = 2)
)
```


\chapter{Problem statement}

> "[The web] has helped broaden the focus of statistics from the modeling stage to all stages of data science: finding relevant data, accessing data, reading and transforming data, visualizing the data in rich ways, modeling, and presenting the results and conclusions with compelling, interactive displays." - [@nolan-lang]

The web enables broad distribution and presentation of applied statistics products and research. Partaking often requires a non-trivial understanding of web technologies, unless a custom interface is designed for the particular task. The CRAN task views on _open data_ [@OpenData] and _web services_ [@WebServices] document such interfaces for the R language, the world's leading open source data science software [@RCore]. This large community effort helps R users make their work easily accessible, portable, and interactive.

R has a long history of serving as an interface to computational facilities for the use of people doing data analysis and statistics research. In fact, the motivation behind the birth of R's predecessor, S, was to provide a direct, consistent, and interactive interface to the best computational facilities already available in languages such as FORTRAN and C [@S:1978]. This empowers users to focus on the primary goal of statistical modeling and data analysis problems, rather than the computational implementation details. By providing more and better interfaces to web services, we can continue to empower R users in a similar way, by making it easier to acquire and/or share data, create interactive web graphics and reports, distribute research products to a large audience in a portable way, and more generally, take advantage of modern web services.

Portability prevents the broad dissemination of statistical computing research, especially interactive statistical graphics. Interactive graphics software traditionally depend on software toolkits like GTK+ or openGL that provide widgets for making interface elements, and also event loops for catching user input. These toolkits need to be installed locally on a user's computer, across various platforms, which adds to installation complexity, impeding portability. Modern web browsers with HTML5 support are now ubiquitous, and provide a cross-platform solution for sharing interactive statistical graphics. However, interfacing web-based visualizations with statistical analysis software remains difficult, and still requires juggling many languages and technologies. By providing better interfaces for creating web-based interactive statistical graphics, we can make them more accessible, and therefore make it easier to share statistical research to a wider audience. This research addresses this gap. 

\chapter{Overview}

# What makes a good statistical software interface?

## Synergy between interfaces

Roughly speaking, there are two broad categories of software interfaces: graphical user interfaces (GUIs) and programming interfaces. Within the domain of statistical computing and data analysis, @Unwin:1999vp explores the strengths and weaknesses of each category, and argues that an effective combination of both is required in order to use statistical software to its full potential. Their main argument is that, since programming interfaces are precise and repeatable, they are preferable when we can describe exactly what we want, but a GUI is better when: "Searching for information and interesting structures without fully specified questions."

@Unwin:1999vp further discuss the different audiences these interfaces tend to attract. Programming interfaces attract power users who need flexibility, such as applied statisticians and statistical researchers in a university, whereas more casual users of statistical software prefer GUIs since they help hide implementation details and allow the focus to be on data analysis. GUIs are still certainly useful for power users in their own work, especially when performing data analysis tasks that are more exploratory (data first, hypothesis second) than confirmatory (hypothesis first, data second) in nature. At the end of the day, all software interfaces are fundamentally _user_ interfaces, and the interface which enables one to do their work most effectively should be preferable.

Unfortunately, as @Unwin:1999vp says, "There is a tendency to judge software by the most powerful tools they provide (whether its a good interface or not), rather than by whether they do the simple things well". This echoes similar thinking found in the Unix philosophy, a well-known set of software engineering principles which derived from work at Bell Laboratories creating the Unix operating system [@unix]; [@unix-philosophy]. The Unix philosophy primarily values interfaces that each do one simple thing, do it well, and most importantly, **work well with each other**. It is all too common that we evaluate interfaces in isolation, but as @eopl writes: "No matter how complex and polished the individual operations are, it is often the quality of the glue that most directly determines the power of the system".

<!-- Bring Unix philosophy to GUIs -->
The next section discusses work that brings this philosophy towards statistical _programming_ interfaces, but it can also be useful to apply this philosophy towards GUI design. The concept of a GUI can be made much more broad than some may realize. For example, most would not think to consider graphical elements of a plot to be elements of a GUI; but for good reason, this is a key feature of many interactive statistical graphics software systems. In other words, interactive graphics could themselves be considered a GUI, which can (in theory) be embedded inside a larger GUI system. For this reason, interactive graphics systems should strive to work well together with other systems so users can leverage their relative strengths in a single GUI.

<!-- Int graphs are GUIs -->
@Buja:1991vh first described direct manipulation (of graphical elements) in multiple linked plots to perform data base queries and visually reveal high-dimensional structure in real-time. @Cook:2007uk argues this framework is preferable to posing data base queries dynamically via a menus, as described by @Ahlberg:1991, and goes on to state that "Multiple linked views are the optimal framework for posing queries about data". @Unwin:1999vp agrees with this perspective, and more generally state that: "The emphasis with GUIs should be on direct manipulation systems and not on menu (indirect manipulation) systems."

<!-- Limitations of a purely GUI approach -->
As with any GUI, any interactive graphics system has its own special set of strengths and limitations that power users are bound to run up against. This is especially true of systems that are entirely GUI-based, as forcing every possible option into a GUI generally reduces its ease of use. The typical way to resolve the problem is to provide a programming interface which allows one to extend its functionality. A great example is the R package __rggobi__ which provides a programming interface to the GUI-based interactive statistical graphics software GGobi -- a descendant of the X-Windows based system XGobi [@ggobi:2007]; [@rggobi]; [@xgobi]. This interface allows users to leverage both the strengths of R (statistical modeling, data manipulation, etc) and GGobi (interactive and dynamic graphics).

<!-- The pipeline -->
Interactive statistical graphics systems require a reactive programming framework that triggers computations upon certain user events. As @Xie:2014co writes, "One challenge in developing interactive statistical applications is the management of the data pipeline, which controls transformations from data to plot." @viewing-pipeline first proposed some general stages that a data pipeline should possess, and others have wrote generally about variations on the pipeline, but there is a suprising lack of writing on implementation details [@plumbing]. Among the literature that does exist, it is usually (implictly) assumed that the software is running as a desktop application on the user's machine. In a web-based approach, special attention should be taken to where computations occur since its much more preferrable to have the system operate entirely in a web browser, rather than making calls to an external web server (to execute R commands, for instance).

<!-- New meaning of extending features -->
The biggest reason for having an interactive graphics system self-contained inside the web browser is that it makes them much easier on users to run, deploy, and share. Taking a web-based approach also opens the possibility of linking views with (a huge number of!) other interactive web graphics systems. This broadens the scope of what is possible when extending an interactive graphics systems by combining multiple systems (classically systems have taken a rather monolithic view on the framework used to link and render graphics). As a result, the only way to allow users to extend such a system is to provide a mechanism for plugging custom methods into certain stages of the pipeline [@orca]; [@ggobi-pipeline-design], but this fails to address other limitations in the overall system. From a user perspective, its much more powerful to be able to combine relative strengths of each system, for example, a combination of GGobi for continous data and MANET for categorical/missing data [@MANET].

<!-- Linking different systems -->
As discussed further in [Multiple linked views](#multiple-linked-views), its now possible to create a standalone web pages with linked views from R within or even between two different interactive graphics systems. @North:1999vi describes a similar framework for linking views on a Windows platform, but provides no examples beyond simple filter/subset operations. Usually what distinguishes interactive _statistical_ graphics from interactive graphics is the ability to perform statistical computations on dynamically changing data (i.e., the data pertaining to the selection). Compared to something like R, the language of the web browser (JavaScript) has very limited resources for doing statistical computing, so interactive web graphics need to interface with other languages in some way to become more statistical.

<!-- Adding R to the mix -->
Numerous R packages which create interactive web graphics have some native support^[Native support here implies that all computations can be performed entirely inside the web browser, without any external calls to a separate process (e.g. R).] for focusing and linking multiple plots, but most are lacking strong native support for the data pipeline necessary to perform statistical computations in real-time within the browser [@animint]; [@plotly]; [@rbokeh]. This is partly by design as JavaScript (the language of the web) has poor support for statistical computing algorithms and requiring an external R process introduces a significant amount of complexity required to view and share visualizations. There are some promising JavaScript projects that are attempting to provide the statistical resources necessary to build pipelines entirely within the browser, but they are still fairly limited [@datalib]; [@vega-dataflow]; [@bayes-js]. 

<!-- Precomputing -->
The data pipeline computes transform(s)/statistic(s) given different input data which represents user selection(s). If the number of selection states is relatively small, it is possible to precompute every possible output (in R), and push the results to the browser, creating a standalone web page. On the other hand, if the number of selection states is large, users may opt into a client-server model (i.e., a web application) where the web browser (client) requests computations to be performed on a web server. Figure \@ref(client-server) shows a visual depiction of this difference in a linked views environment. There are a number of ways to request R computations from a web browser [@opencpu]; [@plumber], but the R package __shiny__ is by far the most popular approach since authors can create web applications entirely within R (without HTML/JavaScript knowlegde) via an elegant reactive programming framework [@shiny].

```{r server-client, echo = FALSE, fig.cap = "A basic visual depiction of linked views in a standalone web page (A) versus a client-server model (B). In some cases (A), linked views can be resolved within a web browser, which generally leads to a better user experience. In other cases (B), updating views may require calls to a web server running special software."}
knitr::include_graphics("images/server-client")
```

<!-- Using R to control/create a GUI -->
At some rudimentary level, a programming interface like **shiny** provides a way to design a graphical interface around programming interface(s). This type of software not only enables people to share their work with others in a user friendly fashion, but it can also make their own work more efficient. In order to be efficient, the time invested to create the GUI must be less than the amount of time it saves by automating the programming task(s). And to empower this type of efficiency, it helps tremendously to have programming interfaces that work well together.

## Synergy among programming interfaces

A typical data analysis workflow involves many different tasks, from data acquistion, import, wrangling, visualization, modeling, and reporting. @r4ds points out the non-linear iteration between wrangling, visualization, and modeling required to derive insights from data. Often times, each stage requires one or more programming interface, and switching from one interface to another can be frustrating since different interfaces often have different philosophies. In some cases, the frustation involved from transitioning from one interface to another is unavoidable, but in most cases, working with a collection of interfaces that share the same underlying principles helps alleviate friction. This is the motivation behind the R package __tidyverse__ which bundles numerous interfaces for doing fundamental data analysis tasks based largely on the tidy data framework [@tidyverse]; [@tidy-data].

<!-- TODO: Not all of tidyverse is "tidy data"...Pure functional programming! -->

<!-- TODO: "Batteries included" approach that Jereon is doing? -->

While tidy tools provide a nice cognitive framework for many common data analysis tasks, sometimes it's necessary to work with messy (i.e., non-rectangular) data structures. In fact, @r4ds argues that 80% of data analysis tasks can be solved with tidy tools while the remaining 20% requires other tools. Probably the most common non-tidy data structure in R is the nested list which is the most flexible and reliable way to represent web-based data structures, such as JSON and XML, in R.

<!--
Before authoring an interface, one should establish the target audience, the class of problems it should address, and loosely define how the interface should actually work. During this process, it may also be helpful to identify your audience as being primarily composed of _software developers_ or _data analysts_. Developers are typically more interested in using the interface to develop novel software or incorporating the functionality into a larger scientific computing environment [@embedded-computing]. In this case, interactive exploration and troubleshooting is not always a luxury, so robust functionality is of utmost importance. On the other hand, analysts interfaces should work well in an interactive environment since this caters to rapid prototyping of ideas and troubleshooting of errors.

Good developer interfaces often make it easier to implement good analyst interfaces. A great recent example of a good developer interface is the R package __Rcpp__, which provides a seamless interface between R with C++ [@Rcpp]. To date, more than 500 R packages use __Rcpp__ to make interfaces that are both expressive and efficient, including the highly influential analyst interfaces such as __tidyr__ and __dplyr__ [@tidy-data]; [@dplyr]. These interfaces help analysts focus on the primary task of wrangling data into a form suitable for visualization and statistical modeling, rather than focusing on the implementation details behind how the transformations are performed. @Donoho:2015tu argues that these interfaces "May have more impact on today’s practice of data analysis than many highly-regarded theoretical statistics papers".

## Usability

As a result, all too often, analysts must spend time gaining the skills of a software developer. Good analyst interfaces often abstract functionality from developer interfaces in a way that allow analysts to focus on their primary task of acquiring/analyzing/modeling/visualizing data, rather than the implementation details. The following focuses on such work with respect to acquiring data from the web and interactive statistical web graphics. 
-->


# Acquiring and wrangling web content in R

## Interfaces for working with web content

R has a rich history of interfacing with web technologies for accomplishing a variety of tasks such as requesting, manipulating, and creating web content. As an important first step, extending ideas from [@Chambers:1999], Brian Ripley implemented the connections interface for file-oriented input/output in R [@Connections]. This interface supports a variety of common transfer protocols (HTTP, HTTPS, FTP), providing access to most files on the web that can be identified with a Uniform Resource Locator (URL). Connection objects are actually external pointers, meaning that, instead of immediately reading the file, they just point to the file, and make no assumptions about the actual contents of the file.

Many functions in the base R distribution for reading data (e.g., `scan`, `read.table`, `read.csv`, etc.) are built on top of connections, and provide additional functionality for parsing well-structured plain-text into basic R data structures (vector, list, data frame, etc.). However, the base R distribution does not provide functionality for parsing common file formats found on the web (e.g., HTML, XML, JSON). In addition, the standard R connection interface provides no support for communicating with web servers beyond a simple HTTP GET request [@Lang:2006us].

The __RCurl__, __XML__, and __RJSONIO__ packages were major contributions that drastically improved our ability to request, manipulate, and create web content from R [@nolan-lang]. The __RCurl__ package provides a suite of high and low level bindings to the C library libcurl, making it possible to transfer files over more network protocols, communicate with web servers (e.g., submit forms, upload files, etc.), process their responses, and handle other details such as redirects and authentication [@RCurl]. The __XML__ package provides low-level bindings to the C library libxml2, making it possible to download, parse, manipulate, and create XML (and HTML) [@XML]. To make this possible, __XML__ also provides some data structures for representing XML in R. The __RJSONIO__ package provides a mapping between R objects and JavaScript Object Notation (JSON) [@RJSONIO]. These packages were heavily used for years, but several newer interfaces have made these tasks easier and more efficient.

The __curl__, __httr__, and __jsonlite__ packages are more modern R interfaces for requesting content on the web and interacting with web servers. The __curl__ package provides a much simpler interface to libcurl that also supports streaming data (useful for transferring large data), and generally has better performance than __RCurl__ [@curl]. The __httr__ package builds on __curl__ and organizes it's functionality around HTTP verbs (GET, POST, etc.) [@httr]. Since most web application programming interfaces (APIs) organize their functionality around these same verbs, it is often very easy to write R bindings to web services with __httr__. The __httr__ package also builds on __jsonlite__ since it provides consistent mappings between R/JSON and most most modern web APIs accept and send messages in JSON format [@jsonlite]. These packages have already had a profound impact on the investment required to interface R with web services, which are useful for many things beyond data acquisition. For example, it is now easy to install R packages hosted on the web (__devtools__), perform cloud computing (__analogsea__), and archive/share computational outputs (__dvn__, __rfigshare__, __RAmazonS3__, __googlesheets__, __rdrop2__, etc.).

The __rvest__ package builds on __httr__ and makes it easy to manipulate content in HTML/XML files [@rvest]. Using __rvest__ in combination with [SelectorGadget](http://selectorgadget.com/), it is often possible to extract structured information (e.g., tables, lists, links, etc) from HTML with almost no knowledge/familiarity with web technologies. The __XML2R__ package has a similar goal of providing an interface to acquire and manipulate XML content into tabular R data structures without any working knowledge of XML/XSLT/XPath [@Sievert:2014a]. As a result, these interfaces reduce the start-up costs required for analysts to acquire data from the web.

Packages such as __XML__, __XML2R__, and __rvest__ can download and parse the source of web pages, which is _static_, but extracting _dynamic_ web content requires additional tools. The R package __rdom__ fills this void and makes it easy to render and access the Document Object Model (DOM) using the headless browsing engine phantomjs [@rdom]. The R package __RSelenium__ can also render dynamic web pages and simulate user actions, but its broad scope and heavy software requirements make it harder to use and less reliable compared to __rdom__ [@RSelenium]. __rdom__ is also designed to work seamlessly with __rvest__, so that one may use the `rdom()` function instead of `read_html()` to render, parse, and return the DOM as HTML (instead of just the HTML page source).

Any combination of these R packages may be useful in acquiring data for personal use and/or providing a higher-level interface to specific data source(s) to increase their accessibility. The next section focuses on such interfaces.

## Interfaces for acquiring data on the web

The web provides access to the world's largest repository of publicly available information and data. This provides a nice _potential_ resource both teaching and practicing applied statistics, but to be practical useful, it often requires a custom interface to make data more accessible. If publishers follow best practices, a custom interface to the data source usually is not needed, but this is rarely the case. Many times structured data is embedded within larger unstructured documents, making it difficult to incorporate into a data analysis workflow. This is especially true of data used to inform downstream web applications, typically in XML and/or JSON format. There are two main ways to make such data more accessible: (1) package, document, and distribute the data itself (2) provide functionality to acquire the data.

If the data source is fairly small, somewhat static, and freely available with an open license, then we can directly provide data via R packaging mechanism. In this case, it is best practice for package authors include scripts used to acquire, transform, and clean the data. This model is especially nice for both teaching and providing examples, since users can easily access data by installing the R package. @rpkgs provides a nice section outlining the details of bundling data with R packages.^[This section is freely available online <http://r-pkgs.had.co.nz/data.html>.]

R packages that just provide functionality to acquire data can be more desirable than bundling it for several reasons. In some cases, it helps avoid legal issues with re-hosting copyrighted data. Furthermore, the source code of R packages can always be inspected, so users can verify the cleaning and transformations performed on the data to ensure its integrity, and suggest changes if necessary. They are also versioned, which makes the data acquisition, and thus any downstream analysis, more reproducible and transparent. It is also possible to handle dynamic data with such interfaces, meaning that new data can be acquired without any change to the underlying source code. As explained in [Taming PITCHf/x Data with XML2R and pitchRx](#taming-pitchfx-data-with-xml2r-and-pitchrx), this is an important quality of the __pitchRx__ R package since new PITCHf/x data is made available on a daily basis.

Perhaps the largest centralized effort in this domain is lead by [rOpenSci](https://ropensci.org), a community of R developers that, at the time of writing, maintains more than 50 packages providing access to scientific data ranging from bird sightings, species occurrence, and even text/metadata from academic publications. This provides a tremendous service to researchers who want to spend their time building models and deriving insights from data, rather than learning the programming skills necessary to acquire and clean it.

It's becoming increasingly clear that "meta" packages that standardize the interface to data acquisition/curation in a particular domain would be tremendously useful. However, it is not clear how such interfaces should be designed. The R package __etl__ is one step in this direction and aims to provide a standardized interface for _any_ data access package that fits into an Extract-Transform-Load paradigm [@etl]. The package provides generic `extract`-`transform`-`load` functions, but requires package authors to write custom `extract`-`transform` methods for the specific data source. In theory, the default `load` method works for any application; as well as other database management operations such as `update` and `clean`.

# Interactive statistical web graphics

## Why interactive graphics?

Unlike computer graphics which focuses on representing reality, virtually; data visualization is about garnering abstract relationships between multiple variables from visual representation. The dimensionality of data, the number of variables can be anything, usually more than 3D, which summons a need to get beyond 2D canvasses for display. Technology enables this, allowing one to link multiple low-dimensional displays in meaningful ways to reveal high-dimensional structure. As demonstrated in Figure \@ref(fig:touR) using the R package __tourbrush__ [@tourbrush], interactive and dynamic statistical graphics allow us to go beyond the constraints of low-dimensional displays to perceive high-dimensional relationships in data.

```{r touR, echo = FALSE, fig.cap = "A video demonstration of interactive and dynamic techniques for visualizing high-dimensional relationships in data using the R package **tourbrush**. You can view this movie online at <https://vimeo.com/148050343>."}
knitr::include_graphics("images/tourbrush")
```

Interactive statistical graphics are a useful tool for descriptive statistics, as well as for building better inferential models. Any statistician is familiar with diagnosing a model by plotting data in the model space (e.g., residual plot, qqplot). This works well for determining if the assumptions of a model are adequate, but rarely suggests that our model neglects important features in the data. To combat this problem, @Wickham:2015ur suggest to plot the model in the data space and use dynamic interactive statistical graphics to do so. Interactive graphics have also proved to be useful for exploratory model analysis, a situation where we have many models to evaluate, compare, and critique [@Unwin:2003uy]; [@Urbanek:2004]; [@Ripley:2004]; [@Unwin:2006]; [@Wickham:2007wq]. With such power comes responsibility that we can verify that visual discoveries are real, and not due to random chance [@Buja:2009hp]; [@Majumder:2013ie].

Even within the statistical graphics community, the term _interactive_ graphics can mean wildly different things to different people [@swayne-klinke]. Some early statistical literature on the topic uses interactive in the sense that a command-line prompt allows users to create graphics on-the-fly [@S:1984]. That is, users enter commands into a prompt, the prompts evaluates the command, and prints the result (known as the read–eval–print loop (REPL)). Modifying a command to generate another variation of a particular result (e.g., to restyle a static plot) can be thought of as a type of interaction that some might call _indirect manipulation_.

Indirect manipulation can be achieved via a GUI or a programming (i.e., command-line) interface. Indirect manipulation from the command-line is more flexible since we have complete control over the commands, but it is also more cumbersome since we must translate our thoughts into code. Indirect manipulation via a GUI is more restrictive, but it helps reduces the gulf of execution (i.e., easier to generate desired output) for end-users [@Hutchins:1985wu]. In this sense, a GUI can be useful, even for experienced programmers, when the command-line interface impedes the primary task of deriving insight from data. 

In many cases, the gulf of execution can be further reduced through direct manipulation. Roughly speaking, within the context of interactive graphics, direct manipulation occurs whenever direct interaction with plotting elements refocuses or reveals new information tied to the event. @ggobi:2007 use the terms dynamic graphics and direct manipulation to characterize "plots that respond in real time to an analyst's queries and change dynamically to re-focus, link to information from other sources, and re-organize information."

Although this thesis focuses on linked views, direct manipulation encompasses many useful techniques that could be used, for example, to re-focus a particular view. A simple example would be directly manipulating the ordering of boxplots to enhance graphical analysis of variance (ANOVA). By default, most plotting libraries sort categories alphabetically, but this is usually not optimal for visual comparison of groups. With a static plotting library such as __ggplot2__, we could indirectly manipulate the default by going back to the command-line, reordering the factor levels of the categorical variables, and regenerate the plot [@ggplot2]. This is flexible and precise since we may order the levels by any measure we wish (e.g., Median, Mean, IQR, etc.), but it would be much quicker and easier if we had a GUI with a drop-down menu for most of the reasonable sorting options. In a general purpose interactive graphics system such as Mondrian, one can use direct manipulation to directly click and drag on the categories to reorder them, making it quick and easy to compare any two groups of interest [@mondrianbook].

The ASA Section on Statistical Computing and Graphics maintains a video library which captures many useful interactive statistical graphics techniques. Several videos show how XGobi (predecessor to GGobi), a dynamic interactive statistical graphics system, can be used to reveal high-dimensional relationships and structures that cannot be easily identified using numerical methods alone [@xgobi].^[For example, <http://stat-graphics.org/movies/xgobi.html> and <http://stat-graphics.org/movies/grand-tour.html>] Another notable video shows how the interactive graphics system Mondrian can be used to quickly find interesting patterns in high-dimensional data using exploratory data analysis (EDA) techniques [@mondrianbook].^[<http://stat-graphics.org/movies/tour-de-france.html>] The most recent video shows how dynamic interactive techniques can help interpret a topic model (a statistical mixture model applied to text data) using the R package __LDAvis__ [@Sievert:2014b], which is the first web-based visualization in the library, and is discussed at depth in [LDAvis: A method for visualizing and interpreting topics](#ldavis-a-method-for-visualizing-and-interpreting-topics).

In order to be practically useful, interactive statistical graphics must be fast, flexible, accessible, portable, and reproducible. In general, over the last 20-30 years interactive graphics systems were fast and flexible, but were generally not easily accessible, portable, or reproducible. The web browser provides a convenient platform to combat these problems. For example, any visualization created with __LDAvis__ can be shared through a Uniform Resource Locator (URL), meaning that anyone with a web browser and an internet connection can view and interact with a visualization. Furthermore, we can link anyone to any possible state of the visualization by encoding selections with a URL fragment identifier. This makes it possible to link readers to an interesting state of a visualization from an external document, while still allowing them to independently explore the same visualization and assess conclusions drawn from it.^[A good example is <http://cpsievert.github.io/LDAvis/reviews/reviews.html>]

## Web graphics

Thanks to the constant evolution and eventual adoption of HTML5 as a web standard, the modern web browser now provides a viable platform for building an interactive statistical graphics systems. HTML5 refers to a collection of technologies, each designed to perform a certain task, that work together in order to present content in a web browser. The Document Object Model (DOM) is a convention for managing all of these technologies to enable _dynamic_ and _interactive_ web pages. Among these technologies, there are several that are especially relevant for interactive web graphics: 

1. HTML: A markup language for structuring and presenting web content.
2. SVG: A markup language for drawing scalable vector graphics.
3. CSS: A language for specifying styling of web content.
4. JavaScript: A language for manipulating web content.

Juggling all of these technologies just to create a simple statistical plot is a tall order. Thankfully, HTML5 technologies are publicly available, and benefit from thriving community of open source developers and volunteers. In the context of web-based visualization, the most influential contribution is Data Driven Documents (D3), a JavaScript library which provides high-level semantics for binding data to web content (e.g., SVG elements) and orchestrating scene updates/transitions [@Bostock:2011]. D3 is wildly successful because is builds upon web standards, without abstracting them away, which fosters customization and interoperability. However, compared to a statistical graphics environments like R, creating basic charts is complicated, and a large amount of code must be customized for each visualization. As a result, web graphics are widely used for presentation graphics (visualization type is known), but are not (yet!) practically useful for exploratory graphics (visualization type is not known).

There are a number of projects attempting to make interactive web graphics more practically useful for ad-hoc data analysis tasks. For statisticians and other people working with data, this means the interface should look and feel like other graphing interfaces in R where many of the rendering details are abstracted away allowing the user to focus on data analysis. The next section discusses some approaches that provide a direct translation of R graphics to a web-based format -- requiring essentially no additional effort by existing R users to take advantage of them. The section afterwards discusses other approaches which design a brand-new R interface for creating web graphics.

## Translating R graphics to the web

There are a few ways to translate R graphics to a web format, such as SVG. R has built-in support for a SVG graphics device, made available through the `svg()` function, but it can be quite slow, which inspired the new __svglite__ package [@svglite]. The `grid.export()` function from the __gridSVG__ package also provides an SVG device, designed specifically for __grid__ graphics (e.g., __ggplot2__, __lattice__, etc.).^[The **gridGraphics** package makes it possible to draw __base__ graphics as __grid__ graphics -- meaning that **gridSVG** can (indirectly) convert any R graphic.]  It adds the ability to retain structured information about grid objects in the SVG output, making it possible to layer on interactive functionality [@gridSVGreport]. The __rvg__ package is a similar project, but implements its own set of graphics devices to support SVG output as well as proprietary XML-based formats (e.g., Word, Powerpoint, XLSX) [@rvg].

A number of projects attempt to layer interactivity on top of a SVG output generated from R code an SVG graphics device. The __SVGAnnotation__ package was the first attempt at post-processing SVG files (created via `svg()`) to add some basic interactivity including: tooltips, animation, and even linked brushing via mouse hover [@SVGAnnotation].^[Unfortunately, this package now serves as a proof of concept as most examples are now broken, and no one has contributed to the project in 3 years.] The __ggiraph__ package is a more modern approach using a similar idea. It uses the __rvg__ package to generate SVG output via a custom graphics device; but focuses specifically on extending __ggplot2__'s interface, and currently has no semantics for linking plots [@ggiraph]. There are also a number of notable projects that layer interactivity on top of SVG output provided by __gridSVG__'s graphics device, including __vdmR__ which enables (very limited support for) linked brushing between __ggplot2__ graphics and __svgPanZoom__ which adds zooming and panning [@vdmR]; [@svgPanZoom]. Translating R graphics at this level is a fundamentally limited approach, however, because it loses information about the raw data and its mapping to visual space.

The __animint__ and __plotly__ packages take a different approach in translating __ggplot2__ graphics to a web format [@animint]; [@plotly]. Instead of translating directly to SVG via __gridSVG__, they extract relevant information from the internal representation of a __ggplot2__ graphic^[For a visual display of the internal representation used to render a __ggplot2__ graph, see my __shiny__ app here <https://gallery.shinyapps.io/ggtree>.], store it in JavaScript Object Notation (JSON), and pass the JSON as an input to a JavaScript function, which then produces a web based visualization. This design pattern is popular among modern web-based graphing libraries, since it separates out _what_ information is contained in the graphic from _how_ to actually draw it. This has a number of advantages; for example, __plotly__ graphics can be rendered in SVG, or using WebGL (based on HTML5 canvas, not SVG) which allows the browser to render many more graphical marks by leveraging the GPU. It also has the advantage of more extensible from R since novel features can be enabled by adding to and/or modifying the underlying data structure (instead of writing ad-hoc JavaScript code to modify the DOM).

Translating existing R graphics to a web-based format is useful for quickly enabling some basic interactivity, but an extension of the underlying graphics interface may be required to enable more advanced features (e.g. linked views). For example, in both __animint__ and __plotly__, we automatically provide tooltips (which reveal more information about each graphical mark on mouse hover) and clickable legends that show/hide graphical marks corresponding to the legend entry. The __animint__ package extends __ggplot2__'s grammar of graphics implementation to enable animations and linked views with relatively small amount of effort required by those familiar with __ggplot2__. This extension is discussed at length in the chapter [Extending ggplot2’s grammar of graphics implementation for linked and dynamic graphics on the web](#animint). The __plotly__ package supersedes __animint__ to support a larger taxonomy of interaction types (e.g., hover, click, click+drag), interaction modes (e.g., dynamically controllable persistent brush), chart types (e.g., 3D surfaces), and even provides a consistent interface for enabling animation/linked-views across graphs created via __ggplot2__ or its own custom (non-ggplot2) interface. These features are discussed in [plotly for R](https://cpsievert.github.io/plotly_book/).

## Interfacing with interactive web graphics

Although it is more onerous for users to learn a new interface, there are a number of advantages to designing a new R interface (that is independent of any translation) to a web graphics system. For one, the translation may require assumptions about internal workings of another system, making it vulnerable to changes in that system. Moreover, a new interface may be designed to take advantage of _all_ the features available in the web graphics system. In some cases, the custom interface can even be used to provide an elegant way to extend the functionality of a translation mechanism, as described in [Extending `ggplotly()`](#extending-ggplotly()).

An early attempt to design an R interface for general purpose interactive web graphics was the R package __rCharts__ whose R interface is heavily inspired by __lattice__ [@rCharts]; [@lattice]. The most innovative part of __rCharts__ was its ability to interface with many different JavaScript graphing libraries using a single R interface. As the number of JavaScript graphing libraries began to explode, it became obvious this was not a sustainable model, as the R package must bundle each JavaScript library that it supports. However, a lot of the infrastructure, such as providing the glue to render plots in various contexts (e.g., the R console, shiny apps, and __rmarkdown__ documents), have evolved into the R package __htmlwidgets__ [@htmlwidgets]. Having built similar bridges for __animint__ and __LDAvis__, I personally know and appreciate the amount of time and effort this package saves other package authors.

The __htmlwidgets__ framework is not constrained to just graphics, it simply provides a set of conventions for authoring web content from R. Numerous JavaScript data visualization libraries are now made available using this framework, and most are designed for particular use cases, such as __leaflet__ for geo-spatial mapping, __dygraphs__ for time-series, and __networkD3__ for networks [@leaflet]; [@dygraphs]; [@networkD3]^[For more examples and information, see <http://www.htmlwidgets.org/> and <http://hafen.github.io/htmlwidgetsgallery/>]. There are also HTML widgets that provide an interface to more general purpose visualization JavaScript libraries such as __plotly__ and __rbokeh__ [@plotly]; [@rbokeh]. 

Many __htmlwidgets__ provide at least some native support for direct manipulation such as identifying (i.e., mousing over points to reveal labels), focusing (i.e., pan and zoom), and sometimes linking multiple views (i.e., animation, brushing over points to highlight points in another view, etc). In some cases, this interactivity is handled by the underlying JavaScript library, but __htmlwidgets__ authors also have the option of layering on additional JavaScript to enable custom features. The R package __plotly__ uses this approach to enable features not found in the underlying JSON specification, such as linked views. To understand how it works, it helps to know about Model-View-Controller (MVC) paradigm, and the different places in which it can appear. 

## Multiple MVC paradigms

The Model-View-Controller (MVC) paradigm is a popular programming technique for designing graphical user interfaces with a minimal amount of dependencies which helps increase responsiveness. Responsiveness is absolutely crucial for interactive graphics since it greatly impacts our ability to make observations, draw generalizations, and generate hypotheses [@2014-latency]. In a MVC paradigm, the model contains all the data and logic necessary to generate view(s). The controller can be thought of as a component on top of a view that listens for certain events, and feeds those events to the model so that the model can update view(s) accordingly. As Figure \@ref(fig:mvc) shows, modern approaches to interfacing R with interactive graphics should have multiple model(s) that reside in different places (depending the event type). 

```{r mvc, echo = FALSE, fig.cap = "Four different MVC paradigms. In all the scenarios, the graph acts as the controller, but the model (i.e., the data and logic which updates the view) exists in different places. In Scenario A, a mouse hover event manipulates the model within the underlying JavaScript library. In Scenario B, a window resizing manipulates the model within the HTMLwidget.resize() method, defined by the widget author. In Scenario C, a mouse hover event manipulates the model within the underlying JavaScript library _and_ a model defined by both the user (in R) and the widget author (in JavaScript). In Scenario D, removing outliers from the raw data may require R code to be executed."}
knitr::include_graphics("images/mvc")
```

For relatively simple interactions, like identification (scenario A/C) and resizing (scenario B) in Figure \@ref(fig:mvc), the model should reside within the web browser. Many JavaScript libraries have a native MVC paradigm for manipulating a single graph, like drawing tooltips on mouse hover (scenario A), but there may be other useful manipulation events that the paradigm doesn't support. This could be result of the fact that the JavaScript library simply does not know about the R interface. For example, resizing should work consistently across various contexts (whether viewing in RStudio, __shiny__ apps, __rmarkdown__ documents, etc). Thankfully the __htmlwidgets__ package provides a JavaScript library that triggers a `resize()` event whenever the window size changes within any of these contexts -- allowing the widget author to focus on the resizing model rather defining all the possible controllers.

For more complicated interactions, the model may have to be defined in R by the user, even when manipulating a single graph. Even when the model is defined in R, that does not necessarily imply the controller requires a hook back to re-execute R code. For example, scenario C in Figure \@ref(fig:mvc) details a situation where the R user has specified that lines should be highlighted upon hover from the R interface, which is not natively supported by the underlying JavaScript graphing library, but was implemented by translating the model from R to JSON and custom JavaScript logic. In fact, all of the examples in the section [linking views without shiny](#linking-views-without-shiny) follow this paradigm. 

A callback to re-execute R code is only necessary if the model requires information that is not already contained in the JSON object and JavaScript logic. So, for scenario D in Figure \@ref(fig:mvc), it is theoretically possible to precompute linear models for every possible view and let the MVC paradigm exist inside the web browser. However, this approach does not scale well, so it is often more practical to have the controller trigger callbacks to R. To provide access to a controller in __shiny__, htmlwidget authors need to leverage the JavaScript function `Shiny.onInputChange()` to inform the __shiny__ server (i.e., the model) about a certain event. Here is a hypothetical and over-simplified example where `view` is an object representing a particular graph/view which has an `on()` method that triggers the execution of a function whenever a `"mouse_hover"` on this view happens.

```javascript
view.on("mouse_hover", function(d) {
  Shiny.onInputChange("mouse_hover", d);
})
```

Users may then subscribe to the event in a __shiny__ app by accessing the event in the server logic (a R function which allows one to access `input` values, manipulate them, and generate output). Assuming the data attached the event (`d`) simply contains the `id` property defined in `scatterplot()`, this is one way you could modify the color of a point by hovering on it.

```r
function(input, output) {
  x <- 1:10
  y <- rnorm(10)
  output$view <- renderWidget({
    isSelected <- x == input$mouse_hover
    scatterplot(x, y, id = x, color = isSelected)
  })
}
```

As previously mentioned, this example is over-simplified for the purpose of demonstrating the basic idea behind subscribing to a controller in __shiny__. A proper implementation should also scope input values by view, so users can distinguish events from different views. The section on [Targeting views](#targeting-views) gives an example of why scoping is important.



## Where should the model reside?

<!--TODO: talk about native shiny approaches -->

There is no doubt the MVC paradigm depicted in scenario D of Figure \@ref(fig:mvc) provides a powerful foundation for interactive statistical graphics. In fact, if interactive web graphics want to replicate and extend classical approaches to the subject, they should follow this paradigm to leverage the statistical facilities that R provides in real-time, but _only_ when it is necessary since introducing . The R package __ggvis__, a reworking of ggplot2's grammar of graphics to incorporate interactivity, is a notable project in this direction [@ggvis].



 Similar to __animint__, __ggvis__ encodes plot specific information as JSON, but instead of writing a JavaScript renderer from the ground up, it uses Vega, a popular JSON schema for creating web-based graphics [@vega]. This limits the flexibility of __ggvis__, but it also drastically reduces the overhead in maintaining such a software project, allowing the focus to be on building a grammar for expressing interactions from R. 

The current version of __ggvis__ uses an old version of vega, before a grammar for interactive graphics was added to its JSON schema [@vega-lite]. In order to respond to user interactions with Vega graphics, __ggvis__ has its own custom JavaScript designed specifically for vega. To enable support for coordinated linked views, it exposes the data pipeline to users via the R package __shiny__, a framework for writing web applications in R [@shiny]. A web application is a website which, when visited by users (aka clients), communicates with a web server. This approach is useful when a website needs to execute code that can not be executed in the web browser (e.g., R code). Figure \@ref(fig:server-client) provides a visual demonstration of this model and its relation to the data pipeline necessary for coordinating linked views.

<!-- TODO: compare pipeline to GGobi -->

```{r pipelines, echo=FALSE, fig.cap="A basic visual depiction of the different approaches to implementing a data pipeline for interactive web graphics. The R packages **ggvis** and **shiny** expose the pipeline to users in R, which requires a web server for viewing. The R package **crosstalk** will allow developers to implement and expose the pipeline on both the server and client levels."}
knitr::include_graphics("images/server-client")
```

Generally speaking, websites that render entirely client-side are more desirable since they are easier to share, more responsive, and require less computational resources to run^[The <http://www.shinyapps.io/> service helps to provide easy access to a __shiny__ server (a web server running special shiny software), so that __shiny__ apps can be shared via a URL.]. However, the client-server approach can be very useful for dynamically performing statistical computations, a key characteristic of most interactive statistical graphics systems. [@FastRWeb] and [@opencpu] also allow us to execute R code on a web server, and retrieve output via HTTP, but __shiny__ is the most heavily used since apps can be written entirely in R using a very powerful, yet approachable, reactive programming framework for handling user events. There are also many convenient shortcuts for creating attractive HTML input forms, making it incredibly easy to go from R script to an web app powered by R that dynamically updates when users alter input values. In other words, __shiny__ makes it quick and easy to write web-based GUIs with support for indirect manipulation.

Historically, an advanced understanding of __shiny__ and JavaScript was required to implement direct manipulation in a __shiny__ app. Recently, __shiny__ added support for retrieving information on user events with static R graphics^[This website shows what information is sent from the client to the server when users interact with plot(s) via mouse event(s) -- <http://shiny.rstudio.com/gallery/plot-interaction-basic.html>], allowing developers to coordinate views in a web app, with no JavaScript involved. This is a powerful tool for R users, but it has its weaknesses. Most importantly, its not clear how to handle interactions when positional scales are categorical (e.g., a bar chart) or how to provide a visual clue that something has been selected.

The touring video in Figure \@ref(fig:touR) purposefully uses __shiny__'s built-in support for brushing to demonstrate the problem with providing a visual clue. This points to the fundamental problem in using non-web-based graphics to implement interactive graphics in a web browser: every time the view updates, the display must be redrawn, resulting in a "glitch" effect. If the plot being brushed used native web graphics (e.g., SVG), it would allow for finer control over how the view updates in response to user interactions and/or dynamic data. On the other hand, since __ggvis__ is web-based, and has special client-side functionality, it knows how to smoothly transition from one frame to the next when provided with new data from the __shiny__ server, which is crucial for constructing a mental model of the data space. Having richer interfaces for generating web-based interactive graphics from R that can share selections, and handle smooth transitions, would make this, and many other examples, generally better.



## Sharing models

The R package __crosstalk__ is a new framework for coordinating arbitrary HTML widgets [@crosstalk]. It provides both an R and a JavaScript API for querying selections, meaning __crosstalk__ powered HTML widgets can work with or without __shiny__, and if implemented carefully by HTML widget authors, provides a means for coordinating multiple HTML widgets without shiny. Generally speaking, __crosstalk__ just provides a standard way to set, store, and access selection values in the browser, so the actual logic for updating views based on the selection value(s) is on the HTML widget author, and this part is far from trivial. In a sense, this project is similar to the work of @North:1999vi, which provides semantics for "snapping together" arbitrary views that are aware of the relational schema, but does so in a web-based environment, rather than requiring a machine running Windows. 

The first HTML widget to leverage __crosstalk__ was @d3scatter, but is currently limited to linked highlighting on scatterplots.^[See, for example, <http://rpubs.com/jcheng/crosstalk-demo>] Currently, there are a couple other R packages with __crosstalk__ support, including __leaflet__ and __listviewer__, but __plotly__ is the only package with some support for the data pipeline and different selection modes (transient vs persistent selection). It also has rich support for interaction types, including mouse hover, click, and multiple types of click+drag selections.

Having HTML widgets that can share selections with each other will be a huge step forward for web-based interactive graphics. With some effort and careful implementation by HTML widget authors, it may be possible to provide sensible defaults for updating views between arbitrary widgets, and users that know some JavaScript will also be able to customize or extend these defaults from R. The __htmlwidgets__ package provides conventions for this, by allowing one to send arbitrary JavaScript functions from R that execute after the widget has rendered in the browser. The biggest problem in implementing coordinated widgets will be in managing data structures, since each widget will likely have its own data structure for representing a selection. In this case, in order to coordinate them, users may have to embed widgets in a shiny app to access and organize selections. This gives users tremendous control over sharing selections, but may limit control over smooth transitions between states of a given widget (a key characteristic of dynamic graphics), and increases the amount of complexity involved in sharing their work.










## Multiple linked views

<!-- Old
A general purpose interactive statistical graphics system should possess many direct manipulation techniques such as identifying (i.e., mousing over points to reveal labels), focusing (i.e., view size adjustment, pan and zoom), linked brushing, etc. However, it is the intricate management of information across multiple views of data in response to user events that is most valuable. Extending ideas from [@viewing-pipeline], [@plumbing] point out that any visualization system with linked views must implement a data pipeline. That is, a "central commander" must be able to handle interaction(s) with a given view, translate its meaning to the data space, and update any linked view(s) accordingly. In order to do so, the commander must know, and be able to compute, function(s) from data to visual space, as well as from visual space to the data. Implementing a pipeline that is fast, general, and able to handle statistical transformations is incredibly difficult. Unfortunately, literature on the implementation of such pipelines is virtually non-existent, but @Xie:2014co provides a nice overview of the implementation details in the R package __cranvas__ [@cranvas].
-->

Multiple linked views is a concept that has existed in many forms within the statistical graphics and information visualization communities for decades [@ggobi:2007]; [@Ahlberg:1997tb]. @Cook:2007uk provides nice motivation for and definition of multiple linked views:

> Multiple linked views are the optimal framework for posing queries about data. A user should be able to pose a query graphically, and a computer should be able to present the response graphically as well. Both query and response should occur in the same visual field. This calls for a mechanism that links the graphical query to the graphical response. A graphical user interface that has such linking mechanisms is an implementation of the notion of "multiple linked views."

In a multiple linked views system, all relevant graphics dynamically update based on meaningful user interaction(s). 


That implies, at the very least, the system must be aware of 

graphical elements that are semantically related -- usually through the data used to generated them. 


In some cases, that implies transformations from data to plot must be embedded in the system, and dynamically re-execute when necessary [@viewing-pipeline]. Furthermore, the system must also be aware of the mapping from 

There are a number of R packages that provide a graphics rendering toolkits with built-in support for multiple linked views. Some are implemented as desktop applications [@rggobi]; [@cranvas]; [@iPlots]; [@loon], while others are implemented within a web-based environment [@animint]; [@ggvis]; [@rbokeh]. In addition to being easier to share, the advantage of using web-based option(s) is that we can link views across different systems. To date, the most versatile tool for linking arbitrary views from R is **shiny** [@shiny], which provides a reactive programming framework for authoring web applications powered by R. [Linking views with shiny](#linking-views-with-shiny) explains how to access plotly events on a shiny server, and informing related views about the events.  

Although **shiny** apps provide a tremendous amount of flexibility when linking views, deploying and sharing shiny apps is way more complicated than a standalone HTML file. When you print a plotly object (or any object built on top of the **htmlwidgets** [@htmlwidgets] infrastructure) it produces a standalone HTML file with some interactivity already enabled (e.g., zoom/pan/tooltip). Moreover, the **plotly** package is unique in the sense that you can link multiple views without shiny in three different ways: inside the same plotly object, link multiple plotly objects, or even link to other htmlwidget packages such as **leaflet** [@leaflet]. Furthermore, since plotly.js has some built-in support for performing statistical summaries, in some cases, we can produce aggregated views of selected data. [Linking views without shiny](#linking-views-with-shiny) explains this framework in detail through a series of examples.

Before exploring the two different approaches for linking views, it can be useful to understand a bit about how interactive graphics systems work, in general. @viewing-pipeline and @plumbing discuss the fundamental elements that all interactive graphics systems must possess -- the most important being the concept of a data-plot-pipeline. As @plumbing states: "A pipeline controls the transformation from data to graphical objects on our screens". All of the software discussed in this work describes systems implemented as desktop applications, where the entire pipeline resides on a single machine. However, the situation becomes more complicated in a web-based environment. Developers have to choose more carefully where computations should occur -- in the browser via `JavaScript` (typically more efficient, and easy to share, but a lack of statistical functionality) or in a statistical programming language like `R` (introduces a complicated infrastructure which compromises usability).

Figure \@ref(fig:server-client) provides a basic visual depiction of the two options to consider when linking views in a web-based environment. [Linking views without shiny](#linking-views-with-shiny) explores cases where the pipeline resides entirely within a client's web-browser, without any calls to a separate process. From a user perspective, this is highly desirable because visualizations are then easily shared and viewed from a single file, without any software requirements (besides a web browser). On the other hand, it is a restrictive environment for statistical computing since we can not directly leverage R's computational facilities.^[If the number of possible selection states is small, it may be possible to pre-compute all possible (statistical) results, and navigate them without recomputing on the fly.] 
On other words, whenever the pipeline involves re-computing a statistical model, or performing a complicated aggregation, the best option is to [link views with shiny](#linking-views-with-shiny).

```{r ggobi-pipeline, echo=FALSE, fig.cap="A comparison of the GGobi pipeline to a hybrid approach to linked views."}
knitr::include_graphics("images/pipeline")
```

The browser-based logic which enables **plotly** to [link views without shiny](#linking-views-without-shiny) is not really "pipeline" in the same sense that interactive statistical graphics software systems, like Orca and GGobi, used the term [@orca]; [@ggobi-pipeline-design]. These systems 



\chapter{Scope}

Explain your contributions to each project

https://github.com/ropensci/plotly/graphs/contributors




<!--
## New challenges

As interactive graphics become more accessible and portable, they are being used more and more for presentation, rather than just a tool for discovery used by experts. Nowhere is this more evident than at major news outlets like the New York Times and The UpShot, where interactive graphics are constantly used in web publications to encourage readers to explore data that supplement a narrative. There are some exceptions to the rule[^12], but all too often, these graphics ignore measures of uncertainty, and instead focus on conveying the most amount of information is the most effective way possible. To some degree, this highlights the difference in goals between the statistical graphics and InfoVis communities [@Gelman:2013et].

[^12]: This report does a good job of demonstrating uncertainty in the Labor Department's monthly jobs report using dynamic interactive graphics <http://www.nytimes.com/2014/05/02/upshot/how-not-to-be-misled-by-the-jobs-report.html>

* How to handle multiple, concurrent users?
  > opencpu and FastRWeb enjoy better overall performance compared to shiny since R sessions are stateless.

* What is missing is something akin to the mutaframe (__mutatr__?), that can work entirely client-side (inside the browser), but can also easily integrate with an R server framework (e.g. __shiny__). 
-->

<!--
Functional programming paradigm works well for computational problems with well defined input/output. With interactive web graphics you want to the output to be dynamic, meaning that users can modify the "inputs" even after the output has been determined.
-->

<!--
Numerous JavaScript charting libraries provide wrappers around D3 to simplify certain charts, but these wrappers are rarely designed with multiple linked views in mind. A few notable exceptions are the JavaScript libraries [crossfilter.js](https://github.com/square/crossfilter) and [dc.js](https://github.com/dc-js/dc.js).
These libraries allow for coordinated linked views, but require a heavy amount of JavaScript code, are limited to a predefined set of chart types, and do not support many statistical computations. 
-->

<!--
## GUI Toolkits

A wide array of GUI toolkits have been available in R for years, and many of them interface to GUI construction libraries written in lower-level languages. A couple fairly recent and popular examples include the __RGtk2__ package which provides R bindings to the GTK+ 2.0 library written C and the __rJava__ package which provides R bindings to Java [@RGtk2]; [@rJava]. More recently, GUI development has moved to the web browser. Probably the most attractive consequence of writing a GUI for the web browser is that users do not have to install any software in order to use the interface. 
-->


<!-- TODO

Historically, open source interactive graphics software is often hard to install and practically impossible to distribute to a wider audience. The web browser provides a viable solution to this problem, as sharing an interactive graphics (and even a specific _state_ of the visualization) can be as easy as sharing a Uniform Resource Locator (URL). The web browser doesn't come without some restrictions; however, since it is impossible to maintain the state of multiple windows, a fundamental characteristic of most interactive graphics software. Fortunately, we can still produce linked views by putting multiple plots in a single window.
-->


<!-- TODO: WHERE DOES THIS GO????

In addition to adding infrastructure for testing __animint__'s renderer, I've made a number of other contributions:

1. Wrote bindings for embedding __animint__ plots inside of knitr/rmarkdown/shiny documents, before the advent of __htmlwidgets__, which provides standard conventions for writing such bindings [@htmlwidgets]. At the time of writing, __htmlwidgets__ can only be rendered from the R console, the R Studio viewer, and using R Markdown (v2). For this reason, we decide to not use __htmlwidgets__ since users may want to incorporate this work into a different workflow. 

2. Wrote `animint2gist`, which uploads an __animint__ visualization as a GitHub gist, which allows users to easily share the visualizations with others via a URL link.

3. Implemented __ggplot2__ facets (i.e., `facet_wrap` and `facet_grid`) as well as the fixed coordinate system (i.e., `coord_fixed`).

4. Mentored and assisted Kevin Ferris during his 2015 Google Summer of Code project where he implemented theming options (i.e., `theme`), legend interactivity, and selectize widgets for selecting values via a drop-down menu.

When I started on __plotly__, it's core functionality and philosophy was very similar to __animint__: create interactive web-based visualizations using __ggplot2__ syntax [@plotly]. However, plotly's `JavaScript` graphing library supports chart types and certain customization that __ggplot2__'s syntax doesn't support. Realizing this, I initiated and designed a new domain-specific language (DSL) for using plotly's `JavaScript` graphing library from R. Although it's design is inspired by __ggplot2__'s `qplot` syntax, the DSL does not rely on __ggplot2__, which is desirable since its functionality won't break when __ggplot2__ internals change.

plotly's 'native' R DSL is heavily influenced by concepts deriving from pure functional programming. The output of a pure function is completely determined by its input(s), and because we don't need any other context about the state of the program, it easy to read and understand the intention of any pure function. When a suite of pure functions are designed around a central object type, we can combine these simple pieces into a pipeline to solve complicated tasks, as is done in many popular R packages such as __dplyr__, __tidyr__, __rvest__, etc [@pipelines].

__plotly__'s pure functions are deliberately designed around data frames so we can conceptualize a visualization as a pipe-able sequence of data transformations, model specifications, and mappings from the data/model space to visual space. With the R package __ggvis__ [@ggvis], one can also mix data transformation and visual specifications in a single pipeline, but it does so by providing S3 methods for __dplyr__'s generic functions, so all data transformations in a __ggvis__ pipeline have to use these generics. By directly modeling visualizations as data frames, __plotly__ removes this restriction that transformation must derive from a generic function, and removes the burden of exporting transformation methods on its developers.

__plotly__ even respects transformations that remove attributes used to track visual properties and data mappings. To demonstrate, in the example below, we plot the raw time series with `plot_ly()`, fit a local polynomial regression with `loess()`, obtain the observation-level characteristics of the model with `augment()` from the __broom__ package, layer on the fitted values to the original plot with `add_trace()`, and add a title with `layout()`. 

```{r, eval = FALSE}
library(plotly)
library(broom)
economics %>%
  transform(rate = unemploy / pop) %>%
  plot_ly(x = date, y = rate, name = "raw") %>%
  loess(rate ~ as.numeric(date), data = .) %>%
  augment() %>%
  add_trace(y = .fitted, name = "smooth") %>%
  layout(title = "Proportion of U.S. population that is unemployed")
```

![](plotly.png)

To make this possible, a special environment within __plotly__'s namespace tracks not only visual mappings/properties, but also the order in which they are specified. So, if a __plotly__ function used to modify a visualization (e.g., `add_trace()` or `layout()`) receives a data frame without any special attributes, it retrieves the last plot created, and modifies that plot. 

__animint__ and __plotly__ could be classified as general purpose software for web-based interactive and dynamic statistical graphics; whereas __LDAvis__, could be classified as software for solving a domain specific problem. The __LDAvis__ package creates an interactive web-based visualization of a topic model fit to a corpus of text data using Latent Dirichlet Allocation (LDA) to assist in interpretation of topics. The visualization itself is written entirely with HTML5 technologies and makes use of the `JavaScript` library d3js [@Bostock:2011] to implement advanced interaction techniques that higher-level tools such as __plotly__, __animint__, and/or __shiny__ do not currently support. 
-->

<!-- TODO
  * Make argument that problem-driven vis requires lower-level tools?
  * Explain which parts I/Kenny did on LDAvis?
    * Designed and authored most of the initial implementation -> https://gallery.shinyapps.io/LDAelife
    * Helped implement the completely client-side application ->
https://cpsievert.github.io/LDAvis/reviews/vis/
-->





